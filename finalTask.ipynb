{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOZ3c6J9oYrE0dTUM8B85s1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f4ac8957cc34802817538bae870f0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_301b5473851b4639a2bcfdea33c8be4d",
              "IPY_MODEL_ce4e0c1b0f784f60a6352017952a1c91",
              "IPY_MODEL_d90858659b04465395e2d347377d7714"
            ],
            "layout": "IPY_MODEL_98b92d21c5504e009d47e359304cff3a"
          }
        },
        "301b5473851b4639a2bcfdea33c8be4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1f55fcffe6349aaa348010d6a546cd0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_19f34d6187a44ae690e0921f85eb5a21",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ce4e0c1b0f784f60a6352017952a1c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee612649ed0446e8af19b2f2dbb2fb27",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63a256c644d945f4af5fce3f73ffcc84",
            "value": 2
          }
        },
        "d90858659b04465395e2d347377d7714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b2c424f5674f50b41278c0821e7878",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3727178669c24f83bddae782cf62352a",
            "value": "‚Äá2/2‚Äá[00:17&lt;00:00,‚Äá‚Äá8.69s/it]"
          }
        },
        "98b92d21c5504e009d47e359304cff3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f55fcffe6349aaa348010d6a546cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f34d6187a44ae690e0921f85eb5a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee612649ed0446e8af19b2f2dbb2fb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a256c644d945f4af5fce3f73ffcc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54b2c424f5674f50b41278c0821e7878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3727178669c24f83bddae782cf62352a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosiki1202/GenAI-app/blob/main/finalTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DuLByvV45Sl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate langchain langchain-core langchain-community langchainhub langchain-mcp-adapters sentencepiece bitsandbytes\n",
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "hf_token = \"\" #ÂêÑËá™„ÅÆHugging Face„ÅÆ„Ç¢„ÇØ„Çª„Çπ„Éà„Éº„ÇØ„É≥„ÇíË®òÂÖ•\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,  # 4bit ÈáèÂ≠êÂåñ„Åß„É°„É¢„É™ÁØÄÁ¥Ñ\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "5f4ac8957cc34802817538bae870f0ce",
            "301b5473851b4639a2bcfdea33c8be4d",
            "ce4e0c1b0f784f60a6352017952a1c91",
            "d90858659b04465395e2d347377d7714",
            "98b92d21c5504e009d47e359304cff3a",
            "d1f55fcffe6349aaa348010d6a546cd0",
            "19f34d6187a44ae690e0921f85eb5a21",
            "ee612649ed0446e8af19b2f2dbb2fb27",
            "63a256c644d945f4af5fce3f73ffcc84",
            "54b2c424f5674f50b41278c0821e7878",
            "3727178669c24f83bddae782cf62352a"
          ]
        },
        "id": "bkI4p8mH7NtA",
        "outputId": "95ee5654-5cb5-4cca-fe02-627e64d563ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f4ac8957cc34802817538bae870f0ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GACOyY7Q9o",
        "outputId": "8aa64668-5565-4632-bcec-7c13d134d5cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List\n",
        "from pydantic import Field, model_validator # PydanticÈñ¢ÈÄ£„Çí„Ç§„É≥„Éù„Éº„Éà\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    # Pydantic v2 „ÅÆË®≠ÂÆö: extra='allow' „ÅßËøΩÂä†„Éï„Ç£„Éº„É´„Éâ„ÇíË®±ÂèØ\n",
        "    model_config = {'extra': 'allow'}\n",
        "\n",
        "    system_prompt: Optional[str] = Field(default=None) # system_prompt„ÇíÊòéÁ§∫ÁöÑ„Å´„Éï„Ç£„Éº„É´„Éâ„Å®„Åó„Å¶ÂÆöÁæ©\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ„É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„Éà„Å®ÁµêÂêà\n",
        "        if self.system_prompt:\n",
        "            full_prompt = f\"{self.system_prompt}\\n{prompt}\"\n",
        "        else:\n",
        "            full_prompt = prompt\n",
        "\n",
        "        output = llm_pipeline(full_prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "        # „É¢„Éá„É´„ÅÆÂá∫Âäõ„Åã„ÇâÂÖ•Âäõ„Éó„É≠„É≥„Éó„ÉàÔºà„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„ÉàÔºã„É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„ÉàÔºâ„ÇíÂâäÈô§\n",
        "        # DeepSeek„É¢„Éá„É´„ÅÆÂ†¥Âêà„ÄÅÂá∫Âäõ„Å´„Éó„É≠„É≥„Éó„Éà„ÅåÂê´„Åæ„Çå„Å™„ÅÑ„Åì„Å®„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅreplace„ÅØ‰∏çË¶Å„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì\n",
        "        # ÂøÖË¶Å„Å´Âøú„Åò„Å¶‰ª•‰∏ã„ÅÆË°å„ÇíË™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
        "        return output.strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"deepseek-local\"\n",
        "\n",
        "# „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„ÇíÊåáÂÆö„Åó„Å¶LLM„ÇíÂàùÊúüÂåñ\n",
        "# „Åì„Åì„Å´„ÅÇ„Å™„Åü„ÅÆ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„ÇíË®òÂÖ•„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
        "system_prompt = \"You are an experienced gamer. Your task is to answer user questions\\\n",
        " and recommend games in a professional and passionate manner by following these steps:\\\n",
        "1. **Search Result Evaluation:** Thoroughly analyze the presented data (game information, reviews, ratings, etc.). \\\n",
        "2. **Priority:** Use the number of positive ratings as the core of your recommendation. \\\n",
        "3. **discription Summary:** Read the extracted game discriptions and provide an in-depth summary of why the game is excellent and what makes it appealing to gamers. \\\n",
        "4. **Recommendation:** Enthusiastically recommend the game to users based on the summarized reasons. \\\n",
        "Please feel free to create all steps using both external and internal knowledge.\"\n",
        "\n",
        "llm = LocalLLM(system_prompt=system_prompt)\n",
        "\n",
        "print(\"LocalLLM with system prompt is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bcbQMb7TVv",
        "outputId": "4df5855e-e02c-440d-c81d-4025dcd258c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLLM with system prompt is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os # „Éá„Éê„ÉÉ„Ç∞Áî®„Å´os„Çí„Ç§„É≥„Éù„Éº„Éà\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# „ÄêStep 0: Áí∞Â¢ÉË®≠ÂÆö„Å®LLM„ÅÆÊ∫ñÂÇô„Äë\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# A100 GPU „ÅåÂà©Áî®ÂèØËÉΩ„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„ÄÅ„Éá„Éê„Ç§„Çπ„ÇíÊ±∫ÂÆö\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"‚úÖ Running on {device}: A100 GPU will be used for embedding.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(f\"‚ö†Ô∏è Running on {device}. GPU not found, using CPU for embedding.\")\n",
        "\n",
        "try:\n",
        "    llm # llmÂ§âÊï∞„ÅåÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç\n",
        "except NameError:\n",
        "    llm = None\n",
        "    print(\"‚ö†Ô∏è LLM (llmÂ§âÊï∞) „ÅØÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇRetrievalQA„ÉÅ„Çß„Éº„É≥„ÅØÊßãÁØâ„Åß„Åç„Åæ„Åõ„Çì„ÄÇ\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# „ÄêStep 1: „Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆË™≠„ÅøËæº„Åø„Å®ÂàÜÂâ≤„Äë\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# ÊåáÂÆö„Åï„Çå„ÅüCSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ („Éï„Ç°„Ç§„É´Âêç„ÅØ„Ç´„É¨„É≥„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´ÈÖçÁΩÆ„Åï„Çå„Å¶„ÅÑ„ÇãÂâçÊèê)\n",
        "csv_file_path = \"steam_games_filtered_final.csv\"\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"‚ùå Error: CSV file not found at {csv_file_path}. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"1. Loading document from {csv_file_path}...\")\n",
        "loader = CSVLoader(csv_file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# „ÉÜ„Ç≠„Çπ„ÉàÂàÜÂâ≤\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"   -> Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# „ÄêStep 2: „Éá„Éº„Çø„ÅÆ„Éô„ÇØ„Éà„É´Âåñ„Å®Chroma VectorStore„ÅÆÊßãÁØâ„Äë\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"2. Initializing SentenceTransformer on {device}...\")\n",
        "embedding = SentenceTransformerEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\",\n",
        "    # ‚òÖ A100 GPU„Çí‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„ÅÆÈáçË¶Å„Å™Ë®≠ÂÆö ‚òÖ\n",
        "    model_kwargs={\"device\": device},\n",
        "    # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíÂ¢ó„ÇÑ„Åô („Éá„Éï„Ç©„É´„Éà„ÅØ32)\n",
        "    encode_kwargs={'batch_size': 32} # ÈÅ©ÂÆúË™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
        ")\n",
        "\n",
        "print(\"3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\")\n",
        "# „Åì„Åì„Åß500MB„ÅÆ„Éá„Éº„Çø„ÇíGPU„Åß„Éô„ÇØ„Éà„É´Âåñ„Åô„ÇãÂá¶ÁêÜ„ÅåÂÆüË°å„Åï„Çå„Åæ„Åô„ÄÇ\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "print(\"   -> VectorStore built successfully.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# „ÄêStep 3: Retrieval QA „ÉÅ„Çß„Éº„É≥‰ΩúÊàê„Äë\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if llm is not None:\n",
        "    # Retrieval QA „ÉÅ„Çß„Éº„É≥„ÅÆ‰ΩúÊàê\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    print(\"4. RetrievalQA chain ready. You can now use qa.run('Your question here')\")\n",
        "else:\n",
        "    print(\"4. RetrievalQA chain skipped because LLM (llm variable) is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjTSAj37pwY",
        "outputId": "72672bea-1f45-458f-89e0-f9b5fed052c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Running on cuda: A100 GPU will be used for embedding.\n",
            "1. Loading document from steam_games_filtered_final.csv...\n",
            "   -> Document split into 8291 chunks.\n",
            "2. Initializing SentenceTransformer on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2061331415.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = SentenceTransformerEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\n",
            "   -> VectorStore built successfully.\n",
            "4. RetrievalQA chain ready. You can now use qa.run('Your question here')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "query = \"„É¢„É≥„Çπ„Çø„Éº„Éè„É≥„Çø„Éº„É©„Ç§„Ç∫„ÅåÂ•Ω„Åç„Å†„ÅåÔºå„ÇØ„É™„Ç¢„Åó„Å¶„Åó„Åæ„Å£„Åü„ÅÆ„ÅßÔºåÊ¨°„ÅÆ„Ç≤„Éº„É†„ÅÆ„Åä„Åô„Åô„ÇÅ„ÅåÁü•„Çä„Åü„ÅÑÔºé\" #Query„ÅØRAG„Å´Ë™≠„ÅøËæº„Åæ„Åõ„ÅüÂÜÖÂÆπ„Å´Âøú„Åò„Å¶ÂêÑËá™„ÅßÂ§âÊõ¥„ÄÇRAG„Å´Ë™≠„ÅøËæº„Åæ„Åõ„ÅüÁü•Ë≠ò„Å´Èñ¢„Åô„ÇãÂïè„ÅÑÂêà„Çè„Åõ„Çí„Åô„Çã„ÄÇ\n",
        "\n",
        "# ÂÆüË°å\n",
        "try:\n",
        "    response = qa.run(query)\n",
        "    print(\"ÂõûÁ≠î:\", response)\n",
        "except RuntimeError as e:\n",
        "    print(\"CUDA„É°„É¢„É™„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇÂØæÂá¶Ê°à:\")\n",
        "    print(\"- „É¢„Éá„É´„Çµ„Ç§„Ç∫„ÇíÁ∏ÆÂ∞è\")\n",
        "    print(\"- „Éà„Éº„ÇØ„É≥Êï∞„ÇíÂà∂Èôê\")\n",
        "    print(\"- Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÇíCPU„Å´Âõ∫ÂÆö\")\n",
        "    print(f\"Ë©≥Á¥∞: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxyNLipPCEWx",
        "outputId": "373678fd-17a9-42e7-9adc-ce54a2e972ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-477229760.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÂõûÁ≠î: You are an experienced gamer. Your task is to answer user questions and recommend games in a professional and passionate manner by following these steps:1. **Search Result Evaluation:** Thoroughly analyze the presented data (game information, reviews, ratings, etc.). 2. **Priority:** Use the number of positive ratings as the core of your recommendation. 3. **discription Summary:** Read the extracted game discriptions and provide an in-depth summary of why the game is excellent and what makes it appealing to gamers. 4. **Recommendation:** Enthusiastically recommend the game to users based on the summarized reasons. Please feel free to create all steps using both external and internal knowledge.\n",
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "short_description: „ÄäËßÖÈïøÁîü„ÄãÊòØ‰∏ÄÊ¨æÊÉ≥Ë¶ÅËøòÂéüÁúüÂÆû‰øÆ‰ªô‰∏ñÁïåÁöÑÂºÄÊîæ‰∏ñÁïåËßíËâ≤ÊâÆÊºîÊ∏∏Êàè„ÄÇ Âú®ËøôÈáåÔºå‰Ω†ÂèØ‰ª•‰ΩìÈ™åÂà∞‰ªéÈõ∂ÂºÄÂßã‰∏ÄÊ≠•Ê≠•ÁßØÊîí‰øÆ‰∏∫Á™ÅÁ†¥Â¢ÉÁïåÁöÑÈÄÜÂ§©‰øÆ‰ªô‰πãÊóÖÔºå‰πüÂèØ‰ª•‰ΩìÈ™åÂà∞ÈÇ£‰∫õÂÖÖÊª°‰øÆ‰ªôÂë≥ÁöÑÊïÖ‰∫ãÂíåÂâßÊÉÖ„ÄÇÂΩìÁÑ∂‰πüÂ∞ë‰∏ç‰∫ÜÂØªÊ±ÇÂ•áÈÅá„ÄÅÊé¢Á¥¢ÁßòÂ¢ÉÔºåÊõ¥ÂèØ‰ª•ÂíåÊ∏∏Êàè‰∏≠ÁöÑËßíËâ≤‰∫íÂä®Ôºå‰∏é‰∫∫‰∏∫ÂñÑÊàñÊòØÊùÄ‰∫∫Â§∫ÂÆùÂÖ®ÈÉΩÁî±‰Ω†Ëá™Â∑±ÊäâÊã©„ÄÇ\n",
            "reviews: \n",
            "developers: ['Chalcedony Network']\n",
            "genres: ['Indie', 'RPG', 'Strategy']\n",
            "positive: 25818\n",
            "negative: 1846\n",
            "\n",
            "short_description: Following the previous one, ‚ÄúFate Seeker II‚Äù character development, thinking and exploration and other elements. You can experience real-time combat without switching scenes, and feel the delightful martial arts anytime.\n",
            "reviews: ‚Äú‰∏çÁæÅ‰∫éÊ≠¶‰æ†Ê∏∏ÊàèÁöÑÊù°Êù°Ê°ÜÊ°ÜÔºåÁî®ÂÆÉ‰∏∞ÂØåÁöÑÂÜÖÂÆπ„ÄÅÁ≤æËá¥ÁöÑÁªÜËäÇÔºå‰∏éÁã¨ÁâπÁöÑÊé¢Ê°à‰∏ªÈ¢òÔºåÊâìÈÄ†Âá∫‰∫Ü‰∏ÄÁßçËøîÁíûÂΩíÁúüÁöÑ‰ΩìÈ™å„ÄÇ‚Äù 8.5 ‚Äì Ê∏∏Ê∞ëÊòüÁ©∫ ‚ÄúÂú®‰∏ÄÊ¨æÊ≠¶‰æ†Ê∏∏Êàè‰∏≠‰∏ÄËæπÂàÄÂÖâÂâëÂΩ±‰∏ÄËæπÊé®ÁêÜÁ†¥Ê°àÁöÑ‰ΩìÈ™åÊòØÂà´Êúâ‰∏ÄÁï™Áã¨ÁâπÈ£éÂë≥ÁöÑ„ÄÇ‚Äù 8.6 ‚Äì Ê∏∏‰æ†ÁΩë ‚ÄúË±êÂØåÁöÑ‰∫íÂãïË¶ÅÁ¥†ÂíåÊõ¥ÊúâÈÇèËºØÁöÑÈóúÂç°Ë®≠Ë®à‰πüËÆìÊé¢Á¥¢ÈÄô‰ª∂‰∫ãÊÉÖËÆäÂæóÊõ¥Âä†ÊúâË∂£ÔºåÊÑèÂ§ñÂÑ™ÁßÄÁöÑÊºîÂá∫ÂíåÈ°çÂ§ñÁöÑÂ∞èÈÅäÊà≤ÂíåÁ∂ìÁáüË¶ÅÁ¥†‰πüÁÇ∫‰ΩúÂìÅÊ∑ªÂ¢û‰∫Ü‰∫õË®±È¢®Âë≥ÔºÅ‚Äù Â∑¥ÂìàÂßÜÁâπ\n",
            "developers: ['Áî≤Â±±ÊûóÂ®õÊ®ÇËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏']\n",
            "genres: ['RPG']\n",
            "positive: 12095\n",
            "\n",
            "name: MONSTER HUNTER RISE\n",
            "release_date: 2022-01-12\n",
            "\n",
            "name: ÂóúË°ÄÂç∞ Bloody Spell\n",
            "release_date: 2022-01-26\n",
            "\n",
            "Question: „É¢„É≥„Çπ„Çø„Éº„Éè„É≥„Çø„Éº„É©„Ç§„Ç∫„ÅåÂ•Ω„Åç„Å†„ÅåÔºå„ÇØ„É™„Ç¢„Åó„Å¶„Åó„Åæ„Å£„Åü„ÅÆ„ÅßÔºåÊ¨°„ÅÆ„Ç≤„Éº„É†„ÅÆ„Åä„Åô„Åô„ÇÅ„ÅåÁü•„Çä„Åü„ÅÑÔºé\n",
            "Helpful Answer: 1. **Search Result Evaluation:** Thoroughly analyze the data provided. The user is a fan of Monster Hunter, specifically the World version, and has just cleared it. They are looking for similar games to continue their enjoyment. The search results include two games: \"MONSTER HUNTER RISE\" and \"ÂóúË°ÄÂç∞ Bloody Spell.\" Both are from the Monster Hunter universe, released around the same time. Both have high positive ratings, with positive ratings at 12095 and 25818 respectively. The user's previous experience with Monster Hunter World suggests they enjoy the gameplay mechanics, monster hunting, and the progression system. 2. **Priority:** The core of the recommendation is based on the number of positive ratings. \"ÂóúË°ÄÂç∞ Bloody Spell\" has a higher positive rating (25818) compared to \"MONSTER HUNTER RISE\" (12095). Therefore, \"ÂóúË°ÄÂç∞ Bloody Spell\" takes priority in recommendation. 3. **discription Summary:** \"ÂóúË°ÄÂç∞ Bloody Spell\" is a continuation of the Monster Hunter series, offering a similar experience to Monster Hunter World. It provides an open world, monster hunting, and progression systems. The game emphasizes real-time combat without scene switching, allowing players to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- Ë®≠ÂÆö ---\n",
        "INPUT_FILE = \"games_march2025_cleaned.csv\"  # ÂÖ•Âäõ„Éï„Ç°„Ç§„É´Âêç\n",
        "OUTPUT_FILE = \"steam_games_filtered_final.csv\" # ÊúÄÁµÇÂá∫Âäõ„Éï„Ç°„Ç§„É´Âêç\n",
        "CUTOFF_DATE = '2015-01-01' # „Åì„ÅÆÊó•‰ªò‰ª•Ââç„ÅÆË°å„ÇíÁ†¥Ê£Ñ„Åó„Åæ„Åô („É™„É™„Éº„ÇπÊó•„Åå„Åì„Çå„Çà„ÇäÂè§„ÅÑ„Ç≤„Éº„É†„ÅØÈô§Â§ñ)\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# üî• ÊäΩÂá∫„Åó„Åü„ÅÑÂàóÂêç„Çí„É™„Çπ„Éà„ÅßÊåáÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ üî•\n",
        "# Êó•‰ªò„Éï„Ç£„É´„Çø„Å´ÂøÖË¶Å„Å™Âàó„Å®„ÄÅ‰øùÊåÅ„Åó„Åü„ÅÑÂàó„ÅÆ„Åø„ÇíÊåáÂÆö\n",
        "COLUMNS_TO_KEEP = [\n",
        "    'release_date',\n",
        "    'name',\n",
        "    'genres',\n",
        "    'short_description',\n",
        "    'detailed_description',\n",
        "    'positive', # positive Âàó„Çí‰øùÊåÅ\n",
        "    'negative',\n",
        "    'reviews',\n",
        "    'developers'\n",
        "]\n",
        "\n",
        "# --- Âá¶ÁêÜ ---\n",
        "\n",
        "def process_csv_for_date_and_positive_filter():\n",
        "    \"\"\"CSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„Åø„ÄÅÊó•‰ªò„Éï„Ç£„É´„Çø„Å®positive„É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„ÇíÈÅ©Áî®„Åó„Å¶‰øùÂ≠ò„Åô„ÇãÈñ¢Êï∞„ÄÇ\"\"\"\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"‚ùå „Ç®„É©„Éº: ÂÖ•Âäõ„Éï„Ç°„Ç§„É´ '{INPUT_FILE}' „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ„Éï„Ç°„Ç§„É´„Åå„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    print(f\"1. „Éï„Ç°„Ç§„É´ '{INPUT_FILE}' „ÇíË™≠„ÅøËæº„Åø„ÄÅÂøÖË¶Å„Å™Âàó„ÇíÊäΩÂá∫„Åó„Åæ„Åô...\")\n",
        "    try:\n",
        "        # 1. ÂøÖË¶Å„Å™Âàó„Å†„Åë„Çí„É°„É¢„É™„Å´Ë™≠„ÅøËæº„ÇÄ (usecols„Å´„Çà„Çã„É°„É¢„É™ÂäπÁéáÂåñ)\n",
        "        # 'release_date'Âàó„ÅØÂøÖ„ÅöÂê´„ÇÅ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô\n",
        "        cols_to_use = [col for col in COLUMNS_TO_KEEP if col in pd.read_csv(INPUT_FILE, encoding=ENCODING, nrows=1).columns]\n",
        "        if 'release_date' not in cols_to_use:\n",
        "             print(f\"‚ùå „Ç®„É©„Éº: CSV„Éï„Ç°„Ç§„É´„Å´ 'release_date' Âàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇÂá¶ÁêÜ„Çí‰∏≠Êñ≠„Åó„Åæ„Åô„ÄÇ\", file=sys.stderr)\n",
        "             return\n",
        "        # positive Âàó„ÅåÂøÖË¶Å„Å™Â†¥Âêà„ÅØËøΩÂä†\n",
        "        if 'positive' not in cols_to_use and 'positive' in COLUMNS_TO_KEEP:\n",
        "             cols_to_use.append('positive')\n",
        "\n",
        "\n",
        "        df = pd.read_csv(INPUT_FILE, encoding=ENCODING, usecols=cols_to_use)\n",
        "\n",
        "        # Âá¶ÁêÜÈñãÂßãÊôÇ„ÅÆÂÖÉ„ÅÆË°åÊï∞„ÇíË®òÈå≤\n",
        "        original_rows_count = len(df)\n",
        "        current_rows_count = original_rows_count\n",
        "\n",
        "        # 2. Êó•‰ªòÂàó„ÅÆÂ§âÊèõ„Å®„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
        "        if 'release_date' in df.columns:\n",
        "            print(f\"2. Êó•‰ªò„Éï„Ç£„É´„Çø„Éº '{CUTOFF_DATE}' „ÇíÈÅ©Áî®„Åó„Åæ„Åô („Åì„ÅÆÊó•‰ªò‰ª•Èôç„ÅÆ„Éá„Éº„Çø„ÅÆ„Åø„ÇíÊÆã„Åó„Åæ„Åô)...\")\n",
        "            # 'release_date'Âàó„ÅÆNaN„ÇíÈô§Â§ñ„Åó„ÄÅÂûã„ÇíÂ§âÊèõ\n",
        "            df.dropna(subset=['release_date'], inplace=True)\n",
        "            df['release_date'] = pd.to_datetime(df['release_date'])\n",
        "\n",
        "            cutoff_datetime = pd.to_datetime(CUTOFF_DATE)\n",
        "\n",
        "            # „É™„É™„Éº„ÇπÊó•„ÅåCUTOFF_DATE„Çà„Çä„ÇÇÊñ∞„Åó„ÅÑË°å„ÇíÊäΩÂá∫\n",
        "            df_filtered_date = df[df['release_date'] >= cutoff_datetime].copy() # >= „Å´Â§âÊõ¥„Åó„Å¶2022Âπ¥„ÇíÂê´„ÇÄ„Çà„ÅÜ„Å´„Åô„Çã\n",
        "            rows_dropped_by_date = current_rows_count - len(df_filtered_date)\n",
        "            current_rows_count = len(df_filtered_date)\n",
        "            print(f\"   -> Êó•‰ªò„Éï„Ç£„É´„Çø„Åß {rows_dropped_by_date} Ë°å„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ\")\n",
        "            print(f\"   -> Êó•‰ªò„Éï„Ç£„É´„ÇøÂæå„ÅÆË°åÊï∞: {current_rows_count} Ë°å\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è 'release_date' Âàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇÊó•‰ªò„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ\")\n",
        "            df_filtered_date = df.copy() # Êó•‰ªò„Éï„Ç£„É´„Çø„Åå„Å™„ÅÑÂ†¥Âêà„ÅØdfÂÖ®‰Ωì„Çí„Ç≥„Éî„Éº\n",
        "\n",
        "\n",
        "        # 3. positive „É¨„Éì„É•„ÉºÊï∞„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
        "        MIN_POSITIVE_REVIEWS = 10000 # ÈñæÂÄ§„ÇíË®≠ÂÆö\n",
        "\n",
        "        if 'positive' in df_filtered_date.columns:\n",
        "             print(f\"3. positive „É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„Éº ({MIN_POSITIVE_REVIEWS}‰ª∂‰ª•‰∏ä) „ÇíÈÅ©Áî®„Åó„Åæ„Åô...\")\n",
        "             # positive Âàó„ÇíÊï∞ÂÄ§Âûã„Å´Â§âÊèõ„Åó„ÄÅNaN„ÇíÂâäÈô§„Åó„Å¶„Åã„Çâ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
        "             df_filtered_date['positive'] = pd.to_numeric(df_filtered_date['positive'], errors='coerce')\n",
        "             df_filtered_date.dropna(subset=['positive'], inplace=True)\n",
        "\n",
        "             df_filtered_final = df_filtered_date[df_filtered_date['positive'] >= MIN_POSITIVE_REVIEWS].copy()\n",
        "             rows_dropped_by_positive = current_rows_count - len(df_filtered_final)\n",
        "             current_rows_count = len(df_filtered_final)\n",
        "             print(f\"   -> positive „É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„Åß {rows_dropped_by_positive} Ë°å„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ\")\n",
        "             print(f\"   -> „Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆÊúÄÁµÇË°åÊï∞: {current_rows_count} Ë°å\")\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è 'positive' Âàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇpositive „É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ\")\n",
        "             df_filtered_final = df_filtered_date.copy() # positive Âàó„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÊó•‰ªò„Éï„Ç£„É´„ÇøÂæå„ÅÆDataFrame„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®\n",
        "\n",
        "\n",
        "        # 4. ÊúÄÁµÇÁöÑ„Å™CSV„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò\n",
        "        print(f\"4. ÊúÄÁµÇÁöÑ„Å™DataFrame„Çí '{OUTPUT_FILE}' „Å®„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô...\")\n",
        "        # COLUMNS_TO_KEEP„ÅßÊåáÂÆö„Åó„ÅüÂàó„Çí„Åù„ÅÆ„Åæ„Åæ‰øùÂ≠ò\n",
        "        df_filtered_final.to_csv(OUTPUT_FILE, index=False, encoding=ENCODING)\n",
        "\n",
        "        # ‰øùÂ≠òÂæå„ÅÆ„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÇíÁ¢∫Ë™ç\n",
        "        final_size_bytes = os.path.getsize(OUTPUT_FILE)\n",
        "        final_size_mb = final_size_bytes / (1024 * 1024)\n",
        "        print(f\"   -> „Éï„Ç°„Ç§„É´ '{OUTPUT_FILE}' „Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü („Çµ„Ç§„Ç∫: {final_size_mb:.2f} MB).\")\n",
        "        print(\"‚úÖ CSVÂá¶ÁêÜÂÆå‰∫Ü„ÄÇ\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå „Ç®„É©„Éº: „Éï„Ç°„Ç§„É´ '{INPUT_FILE}' „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Âá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}\", file=sys.stderr)\n",
        "\n",
        "# Èñ¢Êï∞„ÇíÂÆüË°å\n",
        "process_csv_for_date_and_positive_filter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6IlVfRN-7Th",
        "outputId": "3f41bcbe-05f4-4ad7-8c98-7ce289dbcd9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. „Éï„Ç°„Ç§„É´ 'games_march2025_cleaned.csv' „ÇíË™≠„ÅøËæº„Åø„ÄÅÂøÖË¶Å„Å™Âàó„ÇíÊäΩÂá∫„Åó„Åæ„Åô...\n",
            "2. Êó•‰ªò„Éï„Ç£„É´„Çø„Éº '2015-01-01' „ÇíÈÅ©Áî®„Åó„Åæ„Åô („Åì„ÅÆÊó•‰ªò‰ª•Èôç„ÅÆ„Éá„Éº„Çø„ÅÆ„Åø„ÇíÊÆã„Åó„Åæ„Åô)...\n",
            "   -> Êó•‰ªò„Éï„Ç£„É´„Çø„Åß 2771 Ë°å„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "   -> Êó•‰ªò„Éï„Ç£„É´„ÇøÂæå„ÅÆË°åÊï∞: 86847 Ë°å\n",
            "3. positive „É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„Éº (10000‰ª∂‰ª•‰∏ä) „ÇíÈÅ©Áî®„Åó„Åæ„Åô...\n",
            "   -> positive „É¨„Éì„É•„ÉºÊï∞„Éï„Ç£„É´„Çø„Åß 85778 Ë°å„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "   -> „Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆÊúÄÁµÇË°åÊï∞: 1069 Ë°å\n",
            "4. ÊúÄÁµÇÁöÑ„Å™DataFrame„Çí 'steam_games_filtered_final.csv' „Å®„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô...\n",
            "   -> „Éï„Ç°„Ç§„É´ 'steam_games_filtered_final.csv' „Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü („Çµ„Ç§„Ç∫: 2.71 MB).\n",
            "‚úÖ CSVÂá¶ÁêÜÂÆå‰∫Ü„ÄÇ\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOZ3c6J9oYrE0dTUM8B85s1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f4ac8957cc34802817538bae870f0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_301b5473851b4639a2bcfdea33c8be4d",
              "IPY_MODEL_ce4e0c1b0f784f60a6352017952a1c91",
              "IPY_MODEL_d90858659b04465395e2d347377d7714"
            ],
            "layout": "IPY_MODEL_98b92d21c5504e009d47e359304cff3a"
          }
        },
        "301b5473851b4639a2bcfdea33c8be4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1f55fcffe6349aaa348010d6a546cd0",
            "placeholder": "​",
            "style": "IPY_MODEL_19f34d6187a44ae690e0921f85eb5a21",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ce4e0c1b0f784f60a6352017952a1c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee612649ed0446e8af19b2f2dbb2fb27",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63a256c644d945f4af5fce3f73ffcc84",
            "value": 2
          }
        },
        "d90858659b04465395e2d347377d7714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b2c424f5674f50b41278c0821e7878",
            "placeholder": "​",
            "style": "IPY_MODEL_3727178669c24f83bddae782cf62352a",
            "value": " 2/2 [00:17&lt;00:00,  8.69s/it]"
          }
        },
        "98b92d21c5504e009d47e359304cff3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f55fcffe6349aaa348010d6a546cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f34d6187a44ae690e0921f85eb5a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee612649ed0446e8af19b2f2dbb2fb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a256c644d945f4af5fce3f73ffcc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54b2c424f5674f50b41278c0821e7878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3727178669c24f83bddae782cf62352a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosiki1202/GenAI-app/blob/main/finalTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DuLByvV45Sl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate langchain langchain-core langchain-community langchainhub langchain-mcp-adapters sentencepiece bitsandbytes\n",
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "hf_token = \"\" #各自のHugging Faceのアクセストークンを記入\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,  # 4bit 量子化でメモリ節約\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "5f4ac8957cc34802817538bae870f0ce",
            "301b5473851b4639a2bcfdea33c8be4d",
            "ce4e0c1b0f784f60a6352017952a1c91",
            "d90858659b04465395e2d347377d7714",
            "98b92d21c5504e009d47e359304cff3a",
            "d1f55fcffe6349aaa348010d6a546cd0",
            "19f34d6187a44ae690e0921f85eb5a21",
            "ee612649ed0446e8af19b2f2dbb2fb27",
            "63a256c644d945f4af5fce3f73ffcc84",
            "54b2c424f5674f50b41278c0821e7878",
            "3727178669c24f83bddae782cf62352a"
          ]
        },
        "id": "bkI4p8mH7NtA",
        "outputId": "95ee5654-5cb5-4cca-fe02-627e64d563ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f4ac8957cc34802817538bae870f0ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GACOyY7Q9o",
        "outputId": "8aa64668-5565-4632-bcec-7c13d134d5cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List\n",
        "from pydantic import Field, model_validator # Pydantic関連をインポート\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    # Pydantic v2 の設定: extra='allow' で追加フィールドを許可\n",
        "    model_config = {'extra': 'allow'}\n",
        "\n",
        "    system_prompt: Optional[str] = Field(default=None) # system_promptを明示的にフィールドとして定義\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # システムプロンプトがある場合は、ユーザープロンプトと結合\n",
        "        if self.system_prompt:\n",
        "            full_prompt = f\"{self.system_prompt}\\n{prompt}\"\n",
        "        else:\n",
        "            full_prompt = prompt\n",
        "\n",
        "        output = llm_pipeline(full_prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "        # モデルの出力から入力プロンプト（システムプロンプト＋ユーザープロンプト）を削除\n",
        "        # DeepSeekモデルの場合、出力にプロンプトが含まれないことがあるため、replaceは不要かもしれません\n",
        "        # 必要に応じて以下の行を調整してください\n",
        "        return output.strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"deepseek-local\"\n",
        "\n",
        "# システムプロンプトを指定してLLMを初期化\n",
        "# ここにあなたのシステムプロンプトを記入してください\n",
        "system_prompt = \"You are an experienced gamer. Your task is to answer user questions\\\n",
        " and recommend games in a professional and passionate manner by following these steps:\\\n",
        "1. **Search Result Evaluation:** Thoroughly analyze the presented data (game information, reviews, ratings, etc.). \\\n",
        "2. **Priority:** Use the number of positive ratings as the core of your recommendation. \\\n",
        "3. **discription Summary:** Read the extracted game discriptions and provide an in-depth summary of why the game is excellent and what makes it appealing to gamers. \\\n",
        "4. **Recommendation:** Enthusiastically recommend the game to users based on the summarized reasons. \\\n",
        "Please feel free to create all steps using both external and internal knowledge.\"\n",
        "\n",
        "llm = LocalLLM(system_prompt=system_prompt)\n",
        "\n",
        "print(\"LocalLLM with system prompt is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bcbQMb7TVv",
        "outputId": "4df5855e-e02c-440d-c81d-4025dcd258c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLLM with system prompt is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os # デバッグ用にosをインポート\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 0: 環境設定とLLMの準備】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# A100 GPU が利用可能であることを確認し、デバイスを決定\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"✅ Running on {device}: A100 GPU will be used for embedding.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(f\"⚠️ Running on {device}. GPU not found, using CPU for embedding.\")\n",
        "\n",
        "try:\n",
        "    llm # llm変数が定義されているか確認\n",
        "except NameError:\n",
        "    llm = None\n",
        "    print(\"⚠️ LLM (llm変数) は定義されていません。RetrievalQAチェーンは構築できません。\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 1: ドキュメントの読み込みと分割】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 指定されたCSVファイルを読み込む (ファイル名はカレントディレクトリに配置されている前提)\n",
        "csv_file_path = \"steam_games_filtered_final.csv\"\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"❌ Error: CSV file not found at {csv_file_path}. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"1. Loading document from {csv_file_path}...\")\n",
        "loader = CSVLoader(csv_file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# テキスト分割\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"   -> Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 2: データのベクトル化とChroma VectorStoreの構築】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"2. Initializing SentenceTransformer on {device}...\")\n",
        "embedding = SentenceTransformerEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\",\n",
        "    # ★ A100 GPUを使用するための重要な設定 ★\n",
        "    model_kwargs={\"device\": device},\n",
        "    # バッチサイズを増やす (デフォルトは32)\n",
        "    encode_kwargs={'batch_size': 32} # 適宜調整してください\n",
        ")\n",
        "\n",
        "print(\"3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\")\n",
        "# ここで500MBのデータをGPUでベクトル化する処理が実行されます。\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "print(\"   -> VectorStore built successfully.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 3: Retrieval QA チェーン作成】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if llm is not None:\n",
        "    # Retrieval QA チェーンの作成\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    print(\"4. RetrievalQA chain ready. You can now use qa.run('Your question here')\")\n",
        "else:\n",
        "    print(\"4. RetrievalQA chain skipped because LLM (llm variable) is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjTSAj37pwY",
        "outputId": "72672bea-1f45-458f-89e0-f9b5fed052c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Running on cuda: A100 GPU will be used for embedding.\n",
            "1. Loading document from steam_games_filtered_final.csv...\n",
            "   -> Document split into 8291 chunks.\n",
            "2. Initializing SentenceTransformer on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2061331415.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = SentenceTransformerEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\n",
            "   -> VectorStore built successfully.\n",
            "4. RetrievalQA chain ready. You can now use qa.run('Your question here')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "query = \"モンスターハンターライズが好きだが，クリアしてしまったので，次のゲームのおすすめが知りたい．\" #QueryはRAGに読み込ませた内容に応じて各自で変更。RAGに読み込ませた知識に関する問い合わせをする。\n",
        "\n",
        "# 実行\n",
        "try:\n",
        "    response = qa.run(query)\n",
        "    print(\"回答:\", response)\n",
        "except RuntimeError as e:\n",
        "    print(\"CUDAメモリエラーが発生しました。対処案:\")\n",
        "    print(\"- モデルサイズを縮小\")\n",
        "    print(\"- トークン数を制限\")\n",
        "    print(\"- 埋め込みモデルをCPUに固定\")\n",
        "    print(f\"詳細: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxyNLipPCEWx",
        "outputId": "373678fd-17a9-42e7-9adc-ce54a2e972ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-477229760.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回答: You are an experienced gamer. Your task is to answer user questions and recommend games in a professional and passionate manner by following these steps:1. **Search Result Evaluation:** Thoroughly analyze the presented data (game information, reviews, ratings, etc.). 2. **Priority:** Use the number of positive ratings as the core of your recommendation. 3. **discription Summary:** Read the extracted game discriptions and provide an in-depth summary of why the game is excellent and what makes it appealing to gamers. 4. **Recommendation:** Enthusiastically recommend the game to users based on the summarized reasons. Please feel free to create all steps using both external and internal knowledge.\n",
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "short_description: 《觅长生》是一款想要还原真实修仙世界的开放世界角色扮演游戏。 在这里，你可以体验到从零开始一步步积攒修为突破境界的逆天修仙之旅，也可以体验到那些充满修仙味的故事和剧情。当然也少不了寻求奇遇、探索秘境，更可以和游戏中的角色互动，与人为善或是杀人夺宝全都由你自己抉择。\n",
            "reviews: \n",
            "developers: ['Chalcedony Network']\n",
            "genres: ['Indie', 'RPG', 'Strategy']\n",
            "positive: 25818\n",
            "negative: 1846\n",
            "\n",
            "short_description: Following the previous one, “Fate Seeker II” character development, thinking and exploration and other elements. You can experience real-time combat without switching scenes, and feel the delightful martial arts anytime.\n",
            "reviews: “不羁于武侠游戏的条条框框，用它丰富的内容、精致的细节，与独特的探案主题，打造出了一种返璞归真的体验。” 8.5 – 游民星空 “在一款武侠游戏中一边刀光剑影一边推理破案的体验是别有一番独特风味的。” 8.6 – 游侠网 “豐富的互動要素和更有邏輯的關卡設計也讓探索這件事情變得更加有趣，意外優秀的演出和額外的小遊戲和經營要素也為作品添增了些許風味！” 巴哈姆特\n",
            "developers: ['甲山林娛樂股份有限公司']\n",
            "genres: ['RPG']\n",
            "positive: 12095\n",
            "\n",
            "name: MONSTER HUNTER RISE\n",
            "release_date: 2022-01-12\n",
            "\n",
            "name: 嗜血印 Bloody Spell\n",
            "release_date: 2022-01-26\n",
            "\n",
            "Question: モンスターハンターライズが好きだが，クリアしてしまったので，次のゲームのおすすめが知りたい．\n",
            "Helpful Answer: 1. **Search Result Evaluation:** Thoroughly analyze the data provided. The user is a fan of Monster Hunter, specifically the World version, and has just cleared it. They are looking for similar games to continue their enjoyment. The search results include two games: \"MONSTER HUNTER RISE\" and \"嗜血印 Bloody Spell.\" Both are from the Monster Hunter universe, released around the same time. Both have high positive ratings, with positive ratings at 12095 and 25818 respectively. The user's previous experience with Monster Hunter World suggests they enjoy the gameplay mechanics, monster hunting, and the progression system. 2. **Priority:** The core of the recommendation is based on the number of positive ratings. \"嗜血印 Bloody Spell\" has a higher positive rating (25818) compared to \"MONSTER HUNTER RISE\" (12095). Therefore, \"嗜血印 Bloody Spell\" takes priority in recommendation. 3. **discription Summary:** \"嗜血印 Bloody Spell\" is a continuation of the Monster Hunter series, offering a similar experience to Monster Hunter World. It provides an open world, monster hunting, and progression systems. The game emphasizes real-time combat without scene switching, allowing players to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- 設定 ---\n",
        "INPUT_FILE = \"games_march2025_cleaned.csv\"  # 入力ファイル名\n",
        "OUTPUT_FILE = \"steam_games_filtered_final.csv\" # 最終出力ファイル名\n",
        "CUTOFF_DATE = '2015-01-01' # この日付以前の行を破棄します (リリース日がこれより古いゲームは除外)\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# 🔥 抽出したい列名をリストで指定してください 🔥\n",
        "# 日付フィルタに必要な列と、保持したい列のみを指定\n",
        "COLUMNS_TO_KEEP = [\n",
        "    'release_date',\n",
        "    'name',\n",
        "    'genres',\n",
        "    'short_description',\n",
        "    'detailed_description',\n",
        "    'positive', # positive 列を保持\n",
        "    'negative',\n",
        "    'reviews',\n",
        "    'developers'\n",
        "]\n",
        "\n",
        "# --- 処理 ---\n",
        "\n",
        "def process_csv_for_date_and_positive_filter():\n",
        "    \"\"\"CSVファイルを読み込み、日付フィルタとpositiveレビュー数フィルタを適用して保存する関数。\"\"\"\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"❌ エラー: 入力ファイル '{INPUT_FILE}' が見つかりません。ファイルがアップロードされているか確認してください。\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    print(f\"1. ファイル '{INPUT_FILE}' を読み込み、必要な列を抽出します...\")\n",
        "    try:\n",
        "        # 1. 必要な列だけをメモリに読み込む (usecolsによるメモリ効率化)\n",
        "        # 'release_date'列は必ず含める必要があります\n",
        "        cols_to_use = [col for col in COLUMNS_TO_KEEP if col in pd.read_csv(INPUT_FILE, encoding=ENCODING, nrows=1).columns]\n",
        "        if 'release_date' not in cols_to_use:\n",
        "             print(f\"❌ エラー: CSVファイルに 'release_date' 列が見つかりません。処理を中断します。\", file=sys.stderr)\n",
        "             return\n",
        "        # positive 列が必要な場合は追加\n",
        "        if 'positive' not in cols_to_use and 'positive' in COLUMNS_TO_KEEP:\n",
        "             cols_to_use.append('positive')\n",
        "\n",
        "\n",
        "        df = pd.read_csv(INPUT_FILE, encoding=ENCODING, usecols=cols_to_use)\n",
        "\n",
        "        # 処理開始時の元の行数を記録\n",
        "        original_rows_count = len(df)\n",
        "        current_rows_count = original_rows_count\n",
        "\n",
        "        # 2. 日付列の変換とフィルタリング\n",
        "        if 'release_date' in df.columns:\n",
        "            print(f\"2. 日付フィルター '{CUTOFF_DATE}' を適用します (この日付以降のデータのみを残します)...\")\n",
        "            # 'release_date'列のNaNを除外し、型を変換\n",
        "            df.dropna(subset=['release_date'], inplace=True)\n",
        "            df['release_date'] = pd.to_datetime(df['release_date'])\n",
        "\n",
        "            cutoff_datetime = pd.to_datetime(CUTOFF_DATE)\n",
        "\n",
        "            # リリース日がCUTOFF_DATEよりも新しい行を抽出\n",
        "            df_filtered_date = df[df['release_date'] >= cutoff_datetime].copy() # >= に変更して2022年を含むようにする\n",
        "            rows_dropped_by_date = current_rows_count - len(df_filtered_date)\n",
        "            current_rows_count = len(df_filtered_date)\n",
        "            print(f\"   -> 日付フィルタで {rows_dropped_by_date} 行を削除しました。\")\n",
        "            print(f\"   -> 日付フィルタ後の行数: {current_rows_count} 行\")\n",
        "        else:\n",
        "            print(\"⚠️ 'release_date' 列が見つかりません。日付フィルタリングはスキップします。\")\n",
        "            df_filtered_date = df.copy() # 日付フィルタがない場合はdf全体をコピー\n",
        "\n",
        "\n",
        "        # 3. positive レビュー数でのフィルタリング\n",
        "        MIN_POSITIVE_REVIEWS = 10000 # 閾値を設定\n",
        "\n",
        "        if 'positive' in df_filtered_date.columns:\n",
        "             print(f\"3. positive レビュー数フィルター ({MIN_POSITIVE_REVIEWS}件以上) を適用します...\")\n",
        "             # positive 列を数値型に変換し、NaNを削除してからフィルタリング\n",
        "             df_filtered_date['positive'] = pd.to_numeric(df_filtered_date['positive'], errors='coerce')\n",
        "             df_filtered_date.dropna(subset=['positive'], inplace=True)\n",
        "\n",
        "             df_filtered_final = df_filtered_date[df_filtered_date['positive'] >= MIN_POSITIVE_REVIEWS].copy()\n",
        "             rows_dropped_by_positive = current_rows_count - len(df_filtered_final)\n",
        "             current_rows_count = len(df_filtered_final)\n",
        "             print(f\"   -> positive レビュー数フィルタで {rows_dropped_by_positive} 行を削除しました。\")\n",
        "             print(f\"   -> フィルタリング後の最終行数: {current_rows_count} 行\")\n",
        "        else:\n",
        "             print(\"⚠️ 'positive' 列が見つかりません。positive レビュー数フィルタリングはスキップします。\")\n",
        "             df_filtered_final = df_filtered_date.copy() # positive 列がない場合は日付フィルタ後のDataFrameをそのまま使用\n",
        "\n",
        "\n",
        "        # 4. 最終的なCSVファイルとして保存\n",
        "        print(f\"4. 最終的なDataFrameを '{OUTPUT_FILE}' として保存します...\")\n",
        "        # COLUMNS_TO_KEEPで指定した列をそのまま保存\n",
        "        df_filtered_final.to_csv(OUTPUT_FILE, index=False, encoding=ENCODING)\n",
        "\n",
        "        # 保存後のファイルサイズを確認\n",
        "        final_size_bytes = os.path.getsize(OUTPUT_FILE)\n",
        "        final_size_mb = final_size_bytes / (1024 * 1024)\n",
        "        print(f\"   -> ファイル '{OUTPUT_FILE}' を保存しました (サイズ: {final_size_mb:.2f} MB).\")\n",
        "        print(\"✅ CSV処理完了。\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ エラー: ファイル '{INPUT_FILE}' が見つかりません。\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 処理中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "\n",
        "# 関数を実行\n",
        "process_csv_for_date_and_positive_filter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6IlVfRN-7Th",
        "outputId": "3f41bcbe-05f4-4ad7-8c98-7ce289dbcd9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ファイル 'games_march2025_cleaned.csv' を読み込み、必要な列を抽出します...\n",
            "2. 日付フィルター '2015-01-01' を適用します (この日付以降のデータのみを残します)...\n",
            "   -> 日付フィルタで 2771 行を削除しました。\n",
            "   -> 日付フィルタ後の行数: 86847 行\n",
            "3. positive レビュー数フィルター (10000件以上) を適用します...\n",
            "   -> positive レビュー数フィルタで 85778 行を削除しました。\n",
            "   -> フィルタリング後の最終行数: 1069 行\n",
            "4. 最終的なDataFrameを 'steam_games_filtered_final.csv' として保存します...\n",
            "   -> ファイル 'steam_games_filtered_final.csv' を保存しました (サイズ: 2.71 MB).\n",
            "✅ CSV処理完了。\n"
          ]
        }
      ]
    }
  ]
}
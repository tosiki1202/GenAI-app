{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOIdMpNJW9M+JxCZNjLbvis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f45e1cec878446848455a1023db1bc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59de41399b9046f290acb1c028c48bae",
              "IPY_MODEL_5652bf4ffd0f4cac813509a49d97f0b0",
              "IPY_MODEL_77cfb608da8b48fc9c666b6c4f6bcae8"
            ],
            "layout": "IPY_MODEL_0a107769bfa74b5389d39cde0e2d55a7"
          }
        },
        "59de41399b9046f290acb1c028c48bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad3fa41c141b413e9d3f80becb7f630a",
            "placeholder": "​",
            "style": "IPY_MODEL_d34bfce901824b80b5ceb1a62704157a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5652bf4ffd0f4cac813509a49d97f0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7cafc6a94154a1f96cc6111aa53d84f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_048faa8824ea4fee9368016c3dcff282",
            "value": 2
          }
        },
        "77cfb608da8b48fc9c666b6c4f6bcae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf93d0faba8f4bda926a839998e64c85",
            "placeholder": "​",
            "style": "IPY_MODEL_b8c1608b1aaf4e73884162786e77cc91",
            "value": " 2/2 [00:17&lt;00:00,  8.33s/it]"
          }
        },
        "0a107769bfa74b5389d39cde0e2d55a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3fa41c141b413e9d3f80becb7f630a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d34bfce901824b80b5ceb1a62704157a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7cafc6a94154a1f96cc6111aa53d84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048faa8824ea4fee9368016c3dcff282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf93d0faba8f4bda926a839998e64c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c1608b1aaf4e73884162786e77cc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosiki1202/GenAI-app/blob/main/finalTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "aDmyVGeIhHOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DuLByvV45Sl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate langchain langchain-core langchain-community langchainhub langchain-mcp-adapters sentencepiece bitsandbytes\n",
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.モデルのロード"
      ],
      "metadata": {
        "id": "PaQD79hIhRAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Define the BitsAndBytesConfig for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config, # Use quantization_config instead of load_in_4bit and torch_dtype\n",
        "    dtype=torch.bfloat16 # Use dtype instead of torch_dtype\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f45e1cec878446848455a1023db1bc9d",
            "59de41399b9046f290acb1c028c48bae",
            "5652bf4ffd0f4cac813509a49d97f0b0",
            "77cfb608da8b48fc9c666b6c4f6bcae8",
            "0a107769bfa74b5389d39cde0e2d55a7",
            "ad3fa41c141b413e9d3f80becb7f630a",
            "d34bfce901824b80b5ceb1a62704157a",
            "c7cafc6a94154a1f96cc6111aa53d84f",
            "048faa8824ea4fee9368016c3dcff282",
            "cf93d0faba8f4bda926a839998e64c85",
            "b8c1608b1aaf4e73884162786e77cc91"
          ]
        },
        "id": "bkI4p8mH7NtA",
        "outputId": "c3a69b5f-b816-42f3-b3fa-dec9c6cee7cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45e1cec878446848455a1023db1bc9d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. csvファイルの整形"
      ],
      "metadata": {
        "id": "oTKBfECYhWHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- 設定 ---\n",
        "INPUT_FILE = \"games_march2025_cleaned.csv\"  # 入力ファイル名\n",
        "OUTPUT_FILE = \"steam_games_filtered_final.csv\" # 最終出力ファイル名\n",
        "CUTOFF_DATE = '2015-01-01' # この日付以前の行を破棄します (リリース日がこれより古いゲームは除外)\n",
        "MIN_POSITIVE_REVIEWS = 7000 # 閾値を設定\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# 🔥 抽出したい列名をリストで指定してください 🔥\n",
        "# 日付フィルタに必要な列と、保持したい列のみを指定\n",
        "COLUMNS_TO_KEEP = [\n",
        "    'release_date',\n",
        "    'name',\n",
        "    'genres',\n",
        "    'price',\n",
        "    'short_description',\n",
        "    'detailed_description',\n",
        "    'positive', # positive 列を保持\n",
        "    'negative',\n",
        "    'developers',\n",
        "    'reviews',\n",
        "    'about_the_game',\n",
        "    'supported_languages'\n",
        "]\n",
        "\n",
        "# --- 処理 ---\n",
        "\n",
        "def process_csv_for_date_and_positive_filter():\n",
        "    \"\"\"CSVファイルを読み込み、日付フィルタとpositiveレビュー数フィルタを適用して保存する関数。\"\"\"\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"❌ エラー: 入力ファイル '{INPUT_FILE}' が見つかりません。ファイルがアップロードされているか確認してください。\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    print(f\"1. ファイル '{INPUT_FILE}' を読み込み、必要な列を抽出します...\")\n",
        "    try:\n",
        "        # 1. 必要な列だけをメモリに読み込む (usecolsによるメモリ効率化)\n",
        "        # 'release_date'列は必ず含める必要があります\n",
        "        cols_to_use = [col for col in COLUMNS_TO_KEEP if col in pd.read_csv(INPUT_FILE, encoding=ENCODING, nrows=1).columns]\n",
        "        if 'release_date' not in cols_to_use:\n",
        "             print(f\"❌ エラー: CSVファイルに 'release_date' 列が見つかりません。処理を中断します。\", file=sys.stderr)\n",
        "             return\n",
        "        # positive 列が必要な場合は追加\n",
        "        if 'positive' not in cols_to_use and 'positive' in COLUMNS_TO_KEEP:\n",
        "             cols_to_use.append('positive')\n",
        "\n",
        "\n",
        "        df = pd.read_csv(INPUT_FILE, encoding=ENCODING, usecols=cols_to_use)\n",
        "\n",
        "        # 処理開始時の元の行数を記録\n",
        "        original_rows_count = len(df)\n",
        "        current_rows_count = original_rows_count\n",
        "\n",
        "        # 2. 日付列の変換とフィルタリング\n",
        "        if 'release_date' in df.columns:\n",
        "            print(f\"2. 日付フィルター '{CUTOFF_DATE}' を適用します (この日付以降のデータのみを残します)...\")\n",
        "            # 'release_date'列のNaNを除外し、型を変換\n",
        "            df.dropna(subset=['release_date'], inplace=True)\n",
        "            df['release_date'] = pd.to_datetime(df['release_date'])\n",
        "\n",
        "            cutoff_datetime = pd.to_datetime(CUTOFF_DATE)\n",
        "\n",
        "            # リリース日がCUTOFF_DATEよりも新しい行を抽出\n",
        "            df_filtered_date = df[df['release_date'] >= cutoff_datetime].copy() # >= に変更して2022年を含むようにする\n",
        "            rows_dropped_by_date = current_rows_count - len(df_filtered_date)\n",
        "            current_rows_count = len(df_filtered_date)\n",
        "            print(f\"   -> 日付フィルタで {rows_dropped_by_date} 行を削除しました。\")\n",
        "            print(f\"   -> 日付フィルタ後の行数: {current_rows_count} 行\")\n",
        "        else:\n",
        "            print(\"⚠️ 'release_date' 列が見つかりません。日付フィルタリングはスキップします。\")\n",
        "            df_filtered_date = df.copy() # 日付フィルタがない場合はdf全体をコピー\n",
        "\n",
        "\n",
        "        # 3. positive レビュー数でのフィルタリング\n",
        "\n",
        "        if 'positive' in df_filtered_date.columns:\n",
        "             print(f\"3. positive レビュー数フィルター ({MIN_POSITIVE_REVIEWS}件以上) を適用します...\")\n",
        "             # positive 列を数値型に変換し、NaNを削除してからフィルタリング\n",
        "             df_filtered_date['positive'] = pd.to_numeric(df_filtered_date['positive'], errors='coerce')\n",
        "             df_filtered_date.dropna(subset=['positive'], inplace=True)\n",
        "\n",
        "             df_filtered_final = df_filtered_date[df_filtered_date['positive'] >= MIN_POSITIVE_REVIEWS].copy()\n",
        "             rows_dropped_by_positive = current_rows_count - len(df_filtered_final)\n",
        "             current_rows_count = len(df_filtered_final)\n",
        "             print(f\"   -> positive レビュー数フィルタで {rows_dropped_by_positive} 行を削除しました。\")\n",
        "             print(f\"   -> フィルタリング後の最終行数: {current_rows_count} 行\")\n",
        "        else:\n",
        "             print(\"⚠️ 'positive' 列が見つかりません。positive レビュー数フィルタリングはスキップします。\")\n",
        "             df_filtered_final = df_filtered_date.copy() # positive 列がない場合は日付フィルタ後のDataFrameをそのまま使用\n",
        "\n",
        "\n",
        "        # 4. 最終的なCSVファイルとして保存\n",
        "        print(f\"4. 最終的なDataFrameを '{OUTPUT_FILE}' として保存します...\")\n",
        "        # COLUMNS_TO_KEEPで指定した列をそのまま保存\n",
        "        df_filtered_final.to_csv(OUTPUT_FILE, index=False, encoding=ENCODING)\n",
        "\n",
        "        # 保存後のファイルサイズを確認\n",
        "        final_size_bytes = os.path.getsize(OUTPUT_FILE)\n",
        "        final_size_mb = final_size_bytes / (1024 * 1024)\n",
        "        print(f\"   -> ファイル '{OUTPUT_FILE}' を保存しました (サイズ: {final_size_mb:.2f} MB).\")\n",
        "        print(\"✅ CSV処理完了。\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ エラー: ファイル '{INPUT_FILE}' が見つかりません。\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 処理中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "\n",
        "# 関数を実行\n",
        "process_csv_for_date_and_positive_filter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l81ZGvlB5uc-",
        "outputId": "cdfefd30-2e09-4fc0-ed3b-7bf04451f673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ファイル 'games_march2025_cleaned.csv' を読み込み、必要な列を抽出します...\n",
            "2. 日付フィルター '2015-01-01' を適用します (この日付以降のデータのみを残します)...\n",
            "   -> 日付フィルタで 2771 行を削除しました。\n",
            "   -> 日付フィルタ後の行数: 86847 行\n",
            "3. positive レビュー数フィルター (7000件以上) を適用します...\n",
            "   -> positive レビュー数フィルタで 85465 行を削除しました。\n",
            "   -> フィルタリング後の最終行数: 1382 行\n",
            "4. 最終的なDataFrameを 'steam_games_filtered_final.csv' として保存します...\n",
            "   -> ファイル 'steam_games_filtered_final.csv' を保存しました (サイズ: 6.17 MB).\n",
            "✅ CSV処理完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LangChain 用ラッパーの定義"
      ],
      "metadata": {
        "id": "r8khopZxhvSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GACOyY7Q9o",
        "outputId": "fb709ec5-c3af-4736-ac00-0e2ccbde157d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List\n",
        "from pydantic import Field, model_validator # Pydantic関連をインポート\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    # Pydantic v2 の設定: extra='allow' で追加フィールドを許可\n",
        "    model_config = {'extra': 'allow'}\n",
        "\n",
        "    system_prompt: Optional[str] = Field(default=None) # system_promptを明示的にフィールドとして定義\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # プロンプトテンプレートを汎用的なものに変更\n",
        "        if self.system_prompt:\n",
        "            # システムプロンプトとユーザープロンプトを結合\n",
        "            full_prompt = f\"<s>[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
        "        else:\n",
        "            full_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "\n",
        "        output = llm_pipeline(full_prompt, max_new_tokens=1024, do_sample=True)[0][\"generated_text\"]\n",
        "\n",
        "        # モデルの出力から入力プロンプト部分と [/INST] を削除してAssistantの応答のみを抽出\n",
        "        # ELYZAモデルの出力形式に合わせて調整\n",
        "        assistant_prefix = \"[/INST]\"\n",
        "        if assistant_prefix in output:\n",
        "            return output.split(assistant_prefix, 1)[1].strip()\n",
        "        else:\n",
        "            # もし[/INST]が出力に含まれない場合は、出力全体を返すか、調整が必要\n",
        "            return output.strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"local-llm\" # タイプ名を汎用的に変更\n",
        "\n",
        "# システムプロンプトを指定してLLMを初期化\n",
        "# ここにあなたのシステムプロンプトを記入してください\n",
        "system_prompt = \"必ず日本語で出力してください．前提条件のみから出力してください．ポジティブなレビューの数と価格，ゲームの説明を必ず行なってください．ユーザの購買意欲を掻き立てるように説明してください．\"\n",
        "llm = LocalLLM(system_prompt=system_prompt)\n",
        "\n",
        "print(\"LocalLLM with system prompt is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bcbQMb7TVv",
        "outputId": "8b9abc0d-792c-46b3-d6dd-9dc7794fc619"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLLM with system prompt is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. RAG チェーンを構築"
      ],
      "metadata": {
        "id": "2Ir0RgjSh_Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os # デバッグ用にosをインポート\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 0: 環境設定とLLMの準備】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# GPU が利用可能であることを確認し、デバイスを決定\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"✅ Running on {device}: GPU will be used for embedding.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(f\"⚠️ Running on {device}. GPU not found, using CPU for embedding.\")\n",
        "\n",
        "try:\n",
        "    llm # llm変数が定義されているか確認\n",
        "except NameError:\n",
        "    llm = None\n",
        "    print(\"⚠️ LLM (llm変数) は定義されていません。RetrievalQAチェーンは構築できません。\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 1: ドキュメントの読み込みと分割】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 指定されたCSVファイルを読み込む (ファイル名はカレントディレクトリに配置されている前提)\n",
        "csv_file_path = \"steam_games_filtered_final.csv\"\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"❌ Error: CSV file not found at {csv_file_path}. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"1. Loading document from {csv_file_path}...\")\n",
        "loader = CSVLoader(csv_file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# テキスト分割\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"   -> Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 2: データのベクトル化とChroma VectorStoreの構築】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"2. Initializing SentenceTransformer on {device}...\")\n",
        "embedding = SentenceTransformerEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\",\n",
        "    model_kwargs={\"device\": device},\n",
        "    # バッチサイズを増やす (デフォルトは32)\n",
        "    encode_kwargs={'batch_size': 64} # 適宜調整してください\n",
        ")\n",
        "\n",
        "print(\"3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "print(\"   -> VectorStore built successfully.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 3: Retrieval QA チェーン作成】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if llm is not None:\n",
        "    # Retrieval QA チェーンの作成\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    print(\"4. RetrievalQA chain ready. You can now use qa.run('Your question here')\")\n",
        "else:\n",
        "    print(\"4. RetrievalQA chain skipped because LLM (llm variable) is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjTSAj37pwY",
        "outputId": "13e7867f-80d5-4aaa-b3dc-4f52bc87bd25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Running on cuda: GPU will be used for embedding.\n",
            "1. Loading document from steam_games_filtered_final.csv...\n",
            "   -> Document split into 1450 chunks.\n",
            "2. Initializing SentenceTransformer on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3510208420.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = SentenceTransformerEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\n",
            "   -> VectorStore built successfully.\n",
            "4. RetrievalQA chain ready. You can now use qa.run('Your question here')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.出力"
      ],
      "metadata": {
        "id": "8_n7ENgqiCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "query = \"日本語対応で，マルチプレイ可能な評価の高いゲームを教えて\" #QueryはRAGに読み込ませた内容に応じて各自で変更。RAGに読み込ませた知識に関する問い合わせをする。\n",
        "\n",
        "# 実行\n",
        "try:\n",
        "    response = qa.run(query)\n",
        "    print(\"回答:\", response)\n",
        "except RuntimeError as e:\n",
        "    print(\"CUDAメモリエラーが発生しました。対処案:\")\n",
        "    print(\"- モデルサイズを縮小\")\n",
        "    print(\"- トークン数を制限\")\n",
        "    print(\"- 埋め込みモデルをCPUに固定\")\n",
        "    print(f\"詳細: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxyNLipPCEWx",
        "outputId": "75b348bf-5960-484e-fb3a-375b6ae7dd9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3925162713.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回答: <think>\n",
            "まず、ユーザーが日本語で、マルチプレイ可能な評価の高いゲームを知りたいと理解しました。まず、提供された4つのゲームの情報を見ると、Name: 魔女的夜宴、Price: 34.99、 supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese']、positive: 8085、negative: 110。このゲームは、3人でプレイ可能で、ユーザーが評価が很高く、positively 8085件、negative 110件です。この(game)がマルチプレイ可能性があり、ユーザーが strongly recommend する可能性があります。\n",
            "\n",
            "次に、Name: Sanfu、Price: 6.59、 supported_languages: ['Simplified Chinese']、positive: 13481、negative: 2821。このゲームは、2人でプレイ可能で、positively 13481件、negative 2821件。ユーザーが strongly recommend する可能性がありますが、support languageはSimplified Chineseのみで、ユーザーが日本語を支持しているため、ダウンロードが容易です。\n",
            "\n",
            "Name: OMORI、Price: 19.99、 supported_languages: ['English', 'Japanese', 'Simplified Chinese', 'Korean']、positive: 75941、negative: 2144。このゲームは、マルチプレイ可能性があり、positively 75941件、negative 2144件。ユーザーが strongly recommend する可能性があります。支持 language は多元一 oats,ユーザーが日本語を支持しているため、ダウンロードが容易です。\n",
            "\n",
            "Name: Senren＊Banka、Price: 19.94、 supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese', 'English']、positive: 29679、negative: 330。このゲームは、マルチプレイ可能性があり、positively 29679件、negative 330件。ユーザーが strongly recommend する可能性があります。支持 language は多元一 oats,ユーザーが日本語を支持しているため、ダウンロードが容易です。\n",
            "\n",
            "したがって、ユーザーが日本語対応で、マルチプレイ可能な評価の高いゲームを知りたい場合、Name: 魔女的夜宴、Name: Sanfu、Name: OMORI、Name: Senren＊Banka が候補です。Name: 魔女的夜宴はpositively 8085件、Name: Sanfuは13481件、Name: OMORIは75941件、Name: Senren＊Bankaは29679件。ユーザーが strongly recommend する可能性があります。ユーザーが strongly recommend する Game として、Name: 魔女的夜宴が最もpositively 8085件で最もhighly rated です。したがって、ユーザーが strongly recommend する Game として、Name: 魔女的夜宴が最適です。\n",
            "</think>\n",
            "\n",
            "Name: 魔女的夜宴  \n",
            "Price: 34.99  \n",
            "supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese']  \n",
            "positive: 8085  \n",
            "negative: 110  \n",
            "\n",
            " strongly recommend!  \n",
            "This game is a Japanese-style romantic visual novel with a high score in multiple awards for its art, music, and characters. It has a multi-player mode and is highly praised for its engaging gameplay and emotional story. The game's popularity among Japanese users is evident, with 8085 positive reviews and only 110 negative reviews. It's a must-play for fans of Japanese visual novels!\n"
          ]
        }
      ]
    }
  ]
}
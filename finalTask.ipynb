{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNu6snnYxGU2VzEymqRJ9md",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "036f3826dcdb4555a4b134d837f28035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e9546b01a3a469daa352c600ff1c80c",
              "IPY_MODEL_305dd63f14aa449ea77ea0bfb4466f6e",
              "IPY_MODEL_61241b0177404451878059080d18c098"
            ],
            "layout": "IPY_MODEL_c09d23c01f1e4556ba2030d6c42d9cf1"
          }
        },
        "9e9546b01a3a469daa352c600ff1c80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f0d8d3e9f941cea61f7cfda5115ae2",
            "placeholder": "​",
            "style": "IPY_MODEL_0c33867230734fac884a98bbaf3142ba",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "305dd63f14aa449ea77ea0bfb4466f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_443c688927fc41d5bdb3e8d8ad0e6943",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f298e05ba63344a882e18d5b8b0d5dcf",
            "value": 2
          }
        },
        "61241b0177404451878059080d18c098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a7c331a91e4e05afd5c8c25657a1b0",
            "placeholder": "​",
            "style": "IPY_MODEL_115e9be1dbc54973b5ddd40ad920fe7c",
            "value": " 2/2 [00:16&lt;00:00,  7.47s/it]"
          }
        },
        "c09d23c01f1e4556ba2030d6c42d9cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f0d8d3e9f941cea61f7cfda5115ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c33867230734fac884a98bbaf3142ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "443c688927fc41d5bdb3e8d8ad0e6943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f298e05ba63344a882e18d5b8b0d5dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30a7c331a91e4e05afd5c8c25657a1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115e9be1dbc54973b5ddd40ad920fe7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosiki1202/GenAI-app/blob/main/finalTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DuLByvV45Sl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate langchain langchain-core langchain-community langchainhub langchain-mcp-adapters sentencepiece bitsandbytes\n",
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"elyza/ELYZA-japanese-Llama-2-7b\"\n",
        "hf_token = \"\" #各自のHugging Faceのアクセストークンを記入\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,  # 4bit 量子化でメモリ節約\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "036f3826dcdb4555a4b134d837f28035",
            "9e9546b01a3a469daa352c600ff1c80c",
            "305dd63f14aa449ea77ea0bfb4466f6e",
            "61241b0177404451878059080d18c098",
            "c09d23c01f1e4556ba2030d6c42d9cf1",
            "35f0d8d3e9f941cea61f7cfda5115ae2",
            "0c33867230734fac884a98bbaf3142ba",
            "443c688927fc41d5bdb3e8d8ad0e6943",
            "f298e05ba63344a882e18d5b8b0d5dcf",
            "30a7c331a91e4e05afd5c8c25657a1b0",
            "115e9be1dbc54973b5ddd40ad920fe7c"
          ]
        },
        "id": "bkI4p8mH7NtA",
        "outputId": "4face04e-ca9f-458b-df6c-6f7139d698c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "036f3826dcdb4555a4b134d837f28035"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_token,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GACOyY7Q9o",
        "outputId": "e636c7b1-1c3b-44ca-f07f-930a93a0e555"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List\n",
        "from pydantic import Field, model_validator # Pydantic関連をインポート\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    # Pydantic v2 の設定: extra='allow' で追加フィールドを許可\n",
        "    model_config = {'extra': 'allow'}\n",
        "\n",
        "    system_prompt: Optional[str] = Field(default=None) # system_promptを明示的にフィールドとして定義\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # システムプロンプトがある場合は、ユーザープロンプトと結合\n",
        "        if self.system_prompt:\n",
        "            full_prompt = f\"{self.system_prompt}\\n{prompt}\"\n",
        "        else:\n",
        "            full_prompt = prompt\n",
        "\n",
        "        output = llm_pipeline(full_prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "        # モデルの出力から入力プロンプト（システムプロンプト＋ユーザープロンプト）を削除\n",
        "        # DeepSeekモデルの場合、出力にプロンプトが含まれないことがあるため、replaceは不要かもしれません\n",
        "        # 必要に応じて以下の行を調整してください\n",
        "        return output.strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"deepseek-local\"\n",
        "\n",
        "# システムプロンプトを指定してLLMを初期化\n",
        "# ここにあなたのシステムプロンプトを記入してください\n",
        "system_prompt = \"あなたは最高のゲームを見つけ出すことに情熱を注ぐ、経験豊富なヘビーゲーマーです。あなたのタスクは、ユーザーからの質問に対し、以下の厳格なステップに従って、プロフェッショナルかつ情熱的な口調でゲームをレコメンドすることです\\\n",
        "1.  **検索結果の評価:** 提示されたデータ（ゲーム情報、レビュー、評価など）を徹底的に分析してください。\\\n",
        "2.  **優先順位:** 必ず「**positive評価の数**」が最も多いゲームを最初に抽出し、レコメンドの核としてください。\\\n",
        "3.  **レビューの要約:** 抽出したゲームのレビュー本文を読み込み、**「なぜそのゲームが優れているのか」「ヘビーゲーマーにとって何が魅力なのか」**という観点で理由を深く要約してください。\\\n",
        "4.  **レコメンド:** ユーザーに対し、要約した理由に基づき、熱意をもってゲームを推薦してください。\"\n",
        "llm = LocalLLM(system_prompt=system_prompt)\n",
        "\n",
        "print(\"LocalLLM with system prompt is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bcbQMb7TVv",
        "outputId": "da7a4f2b-1362-4f7c-ffe5-81184418776f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLLM with system prompt is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os # デバッグ用にosをインポート\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 0: 環境設定とLLMの準備】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# A100 GPU が利用可能であることを確認し、デバイスを決定\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"✅ Running on {device}: A100 GPU will be used for embedding.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(f\"⚠️ Running on {device}. GPU not found, using CPU for embedding.\")\n",
        "\n",
        "try:\n",
        "    llm # llm変数が定義されているか確認\n",
        "except NameError:\n",
        "    llm = None\n",
        "    print(\"⚠️ LLM (llm変数) は定義されていません。RetrievalQAチェーンは構築できません。\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 1: ドキュメントの読み込みと分割】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 指定されたCSVファイルを読み込む (ファイル名はカレントディレクトリに配置されている前提)\n",
        "csv_file_path = \"steam_games_filtered_final.csv\"\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"❌ Error: CSV file not found at {csv_file_path}. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"1. Loading document from {csv_file_path}...\")\n",
        "loader = CSVLoader(csv_file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# テキスト分割\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"   -> Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 2: データのベクトル化とChroma VectorStoreの構築】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"2. Initializing SentenceTransformer on {device}...\")\n",
        "embedding = SentenceTransformerEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\",\n",
        "    # ★ A100 GPUを使用するための重要な設定 ★\n",
        "    model_kwargs={\"device\": device},\n",
        "    # バッチサイズを増やす (デフォルトは32)\n",
        "    encode_kwargs={'batch_size': 32} # 適宜調整してください\n",
        ")\n",
        "\n",
        "print(\"3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\")\n",
        "# ここで500MBのデータをGPUでベクトル化する処理が実行されます。\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "print(\"   -> VectorStore built successfully.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 【Step 3: Retrieval QA チェーン作成】\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if llm is not None:\n",
        "    # Retrieval QA チェーンの作成\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    print(\"4. RetrievalQA chain ready. You can now use qa.run('Your question here')\")\n",
        "else:\n",
        "    print(\"4. RetrievalQA chain skipped because LLM (llm variable) is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjTSAj37pwY",
        "outputId": "94a325f1-64d3-4509-a152-965524ea4db9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Running on cuda: A100 GPU will be used for embedding.\n",
            "1. Loading document from steam_games_filtered_final.csv...\n",
            "   -> Document split into 23292 chunks.\n",
            "2. Initializing SentenceTransformer on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2061331415.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = SentenceTransformerEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\n",
            "   -> VectorStore built successfully.\n",
            "4. RetrievalQA chain ready. You can now use qa.run('Your question here')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "query = \"アクションゲームでおすすめは？\" #QueryはRAGに読み込ませた内容に応じて各自で変更。RAGに読み込ませた知識に関する問い合わせをする。\n",
        "\n",
        "# 実行\n",
        "try:\n",
        "    response = qa.run(query)\n",
        "    print(\"回答:\", response)\n",
        "except RuntimeError as e:\n",
        "    print(\"CUDAメモリエラーが発生しました。対処案:\")\n",
        "    print(\"- モデルサイズを縮小\")\n",
        "    print(\"- トークン数を制限\")\n",
        "    print(\"- 埋め込みモデルをCPUに固定\")\n",
        "    print(f\"詳細: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxyNLipPCEWx",
        "outputId": "668262a0-597c-4425-860d-cfb1a09a478c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1417115627.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回答: あなたは最高のゲームを見つけ出すことに情熱を注ぐ、経験豊富なヘビーゲーマーです。あなたのタスクは、ユーザーからの質問に対し、以下の厳格なステップに従って、プロフェッショナルかつ情熱的な口調でゲームをレコメンドすることです1.  **検索結果の評価:** 提示されたデータ（ゲーム情報、レビュー、評価など）を徹底的に分析してください。2.  **優先順位:** 必ず「**positive評価の数**」が最も多いゲームを最初に抽出し、レコメンドの核としてください。3.  **レビューの要約:** 抽出したゲームのレビュー本文を読み込み、**「なぜそのゲームが優れているのか」「ヘビーゲーマーにとって何が魅力なのか」**という観点で理由を深く要約してください。4.  **レコメンド:** ユーザーに対し、要約した理由に基づき、熱意をもってゲームを推薦してください。\n",
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "name: カンフービート\n",
            "release_date: 2025-02-19\n",
            "short_description: 少女が最強の拳法家を目指す！リズムに乗ってレッツカンフー！ カンフービートは2Dリズムアクションゲームです タイミングよくボタンを押して移動、ジャンプ、攻撃！ 立ちふさがる強敵たち。行動を見切って拳を叩き込め！ 獲得した技や構えを駆使して戦いを有利に進めよう！\n",
            "reviews: \n",
            "genres: ['Action', 'Indie']\n",
            "positive: 9\n",
            "negative: 0\n",
            "\n",
            "name: だっかん！モンスターの島\n",
            "release_date: 2024-02-24\n",
            "short_description: 自分の城を守りつつ、モンスターを出撃させて、敵の城を攻めよう！ モンスターの進むルートを指定できたり、壁を掘って地形を変えたりして自分自身の戦い方でステージを攻略できます。 　モンスターは進んだルートの形に応じてスキルが発動します。 ルートの組み方を工夫することで、スキルを多く発動させたり、最短ルートを指示して敵の城へ素早く攻撃を仕掛けることもできます。 ルートのとり方によって戦略は無限大！ 　モンスターのレベルアップ、世界観がわかるストーリーの収集、キャラクターやロボットのこぼれ話、隠しキャラクターなどやりこみ要素が満載！ 登場キャラクターは１２種類、収録曲は１６曲以上！\n",
            "reviews: \n",
            "genres: ['Action', 'Strategy']\n",
            "positive: 0\n",
            "negative: 0\n",
            "\n",
            "name: ナズーリンの探しもの？\n",
            "release_date: 2024-05-25\n",
            "short_description: ナズーリンが探しものを求めてどこへゆく・・・？ 東方二次創作ゲーム【ナズーリンの探しもの？】です。 2Dアクションゲーム、ナズーリンを操作し、速いタイムでゴールを目指しましょう！\n",
            "reviews: \n",
            "genres: ['Action']\n",
            "positive: 6\n",
            "negative: 0\n",
            "\n",
            "name: 選んで！打って！！クイズ4択タイピング\n",
            "release_date: 2025-02-07\n",
            "short_description: 解答方法を4択またはタイピングから選んで遊ぶ早押しクイズ！4人で対戦するランダムマッチか、ルールを自由に設定して対戦するルームマッチに参加して誰よりも早く解答しよう！\n",
            "reviews: \n",
            "genres: ['Casual', 'Indie']\n",
            "positive: 0\n",
            "negative: 0\n",
            "\n",
            "Question: アクションゲームでおすすめは？\n",
            "Helpful Answer: 【モンスターの島】 【だっかん！\n",
            "モンスターの島】 【ナズーリンの探しもの？】 【選んで！\n",
            "打って！\n",
            "！\n",
            "クイズ4択タイピング】 【カンフービート】 【あなたは最高のゲームを見つけることができるか？】 【あなたは最高のゲームを見つけることができるか？】\n",
            "name: 悪魔の迷宮\n",
            "release_date: 2024-05-25\n",
            "short_description: 悪魔の迷宮を探索し、悪魔の遺物を奪取する早押しクイズ！\n",
            "早押しクイズゲーム【悪魔の迷宮】です。\n",
            "2Dアクションゲーム、悪魔の��\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- 設定 ---\n",
        "INPUT_FILE = \"games_march2025_cleaned.csv\"  # 入力ファイル名\n",
        "OUTPUT_FILE = \"steam_games_filtered_final.csv\" # 最終出力ファイル名\n",
        "CUTOFF_DATE = '2024-1-1' # この日付以前の行を破棄します\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# 🔥 抽出したい列名をリストで指定してください 🔥\n",
        "# あなたのCSVファイルのカラム名に合わせて修正が必要です\n",
        "COLUMNS_TO_KEEP = [\n",
        "    'name',\n",
        "    'release_date',\n",
        "    'genres',\n",
        "    'short_description',\n",
        "    'positive',\n",
        "    'negative',\n",
        "    'reviews'\n",
        "]\n",
        "\n",
        "# --- 処理 ---\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"❌ エラー: 入力ファイル '{INPUT_FILE}' が見つかりません。ファイルがアップロードされているか確認してください。\")\n",
        "else:\n",
        "    print(f\"1. ファイル '{INPUT_FILE}' を読み込み、必要な列を抽出します...\")\n",
        "    try:\n",
        "        # 1. 必要な列だけをメモリに読み込む (usecolsによるメモリ効率化)\n",
        "        # 'release_date'列は必ず含める必要があります\n",
        "        df = pd.read_csv(INPUT_FILE, encoding=ENCODING, usecols=COLUMNS_TO_KEEP)\n",
        "\n",
        "        # 2. 日付列をdatetime型に変換し、フィルタリング前の行数を記録\n",
        "        df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
        "        rows_before_filter = len(df)\n",
        "\n",
        "        # 3. フィルタリングを実行: CUTOFF_DATEよりも新しい行を保持\n",
        "        print(f\"2. 日付フィルター '{CUTOFF_DATE}' を適用します...\")\n",
        "        cutoff_datetime = pd.to_datetime(CUTOFF_DATE)\n",
        "\n",
        "        # リリース日がCUTOFF_DATEよりも大きい行を抽出\n",
        "        df_filtered = df[df['release_date'] > cutoff_datetime].copy()\n",
        "\n",
        "        # 4. 新しいファイルとして保存\n",
        "        df_filtered.to_csv(OUTPUT_FILE, index=False, encoding=ENCODING)\n",
        "\n",
        "        # 5. 結果の確認\n",
        "        rows_after_filter = len(df_filtered)\n",
        "        final_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
        "\n",
        "        print(\"\\n🎉 処理が完了しました！\")\n",
        "        print(f\"   元の行数: {rows_before_filter} 行\")\n",
        "        print(f\"   最終的な行数: {rows_after_filter} 行\")\n",
        "        print(f\"   出力ファイル名: '{OUTPUT_FILE}'\")\n",
        "        print(f\"   最終ファイルサイズ: {final_size_mb:.2f} MB\")\n",
        "\n",
        "        if final_size_mb > 10.0:\n",
        "            print(\"⚠️ 注意: 最終ファイルサイズが10MBを超えています。RAG向けにさらに行数を制限する必要があるかもしれません。\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ エラー: CSVファイルに指定された列 '{e}' が見つかりません。`COLUMNS_TO_KEEP`を確認してください。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ファイル処理中に致命的なエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6IlVfRN-7Th",
        "outputId": "3f0f9167-7096-4ea0-ff53-bbdae77f92d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ファイル 'games_march2025_cleaned.csv' を読み込み、必要な列を抽出します...\n",
            "2. 日付フィルター '2024-1-1' を適用します...\n",
            "\n",
            "🎉 処理が完了しました！\n",
            "   元の行数: 89618 行\n",
            "   最終的な行数: 21832 行\n",
            "   出力ファイル名: 'steam_games_filtered_final.csv'\n",
            "   最終ファイルサイズ: 6.18 MB\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOIdMpNJW9M+JxCZNjLbvis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f45e1cec878446848455a1023db1bc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59de41399b9046f290acb1c028c48bae",
              "IPY_MODEL_5652bf4ffd0f4cac813509a49d97f0b0",
              "IPY_MODEL_77cfb608da8b48fc9c666b6c4f6bcae8"
            ],
            "layout": "IPY_MODEL_0a107769bfa74b5389d39cde0e2d55a7"
          }
        },
        "59de41399b9046f290acb1c028c48bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad3fa41c141b413e9d3f80becb7f630a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d34bfce901824b80b5ceb1a62704157a",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "5652bf4ffd0f4cac813509a49d97f0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7cafc6a94154a1f96cc6111aa53d84f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_048faa8824ea4fee9368016c3dcff282",
            "value": 2
          }
        },
        "77cfb608da8b48fc9c666b6c4f6bcae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf93d0faba8f4bda926a839998e64c85",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8c1608b1aaf4e73884162786e77cc91",
            "value": "â€‡2/2â€‡[00:17&lt;00:00,â€‡â€‡8.33s/it]"
          }
        },
        "0a107769bfa74b5389d39cde0e2d55a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3fa41c141b413e9d3f80becb7f630a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d34bfce901824b80b5ceb1a62704157a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7cafc6a94154a1f96cc6111aa53d84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048faa8824ea4fee9368016c3dcff282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf93d0faba8f4bda926a839998e64c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c1608b1aaf4e73884162786e77cc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosiki1202/GenAI-app/blob/main/finalTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ],
      "metadata": {
        "id": "aDmyVGeIhHOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1DuLByvV45Sl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate langchain langchain-core langchain-community langchainhub langchain-mcp-adapters sentencepiece bitsandbytes\n",
        "!pip install chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰"
      ],
      "metadata": {
        "id": "PaQD79hIhRAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Define the BitsAndBytesConfig for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config, # Use quantization_config instead of load_in_4bit and torch_dtype\n",
        "    dtype=torch.bfloat16 # Use dtype instead of torch_dtype\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f45e1cec878446848455a1023db1bc9d",
            "59de41399b9046f290acb1c028c48bae",
            "5652bf4ffd0f4cac813509a49d97f0b0",
            "77cfb608da8b48fc9c666b6c4f6bcae8",
            "0a107769bfa74b5389d39cde0e2d55a7",
            "ad3fa41c141b413e9d3f80becb7f630a",
            "d34bfce901824b80b5ceb1a62704157a",
            "c7cafc6a94154a1f96cc6111aa53d84f",
            "048faa8824ea4fee9368016c3dcff282",
            "cf93d0faba8f4bda926a839998e64c85",
            "b8c1608b1aaf4e73884162786e77cc91"
          ]
        },
        "id": "bkI4p8mH7NtA",
        "outputId": "c3a69b5f-b816-42f3-b3fa-dec9c6cee7cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45e1cec878446848455a1023db1bc9d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. csvãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´å½¢"
      ],
      "metadata": {
        "id": "oTKBfECYhWHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "INPUT_FILE = \"games_march2025_cleaned.csv\"  # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å\n",
        "OUTPUT_FILE = \"steam_games_filtered_final.csv\" # æœ€çµ‚å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å\n",
        "CUTOFF_DATE = '2015-01-01' # ã“ã®æ—¥ä»˜ä»¥å‰ã®è¡Œã‚’ç ´æ£„ã—ã¾ã™ (ãƒªãƒªãƒ¼ã‚¹æ—¥ãŒã“ã‚Œã‚ˆã‚Šå¤ã„ã‚²ãƒ¼ãƒ ã¯é™¤å¤–)\n",
        "MIN_POSITIVE_REVIEWS = 7000 # é–¾å€¤ã‚’è¨­å®š\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# ğŸ”¥ æŠ½å‡ºã—ãŸã„åˆ—åã‚’ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¦ãã ã•ã„ ğŸ”¥\n",
        "# æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã«å¿…è¦ãªåˆ—ã¨ã€ä¿æŒã—ãŸã„åˆ—ã®ã¿ã‚’æŒ‡å®š\n",
        "COLUMNS_TO_KEEP = [\n",
        "    'release_date',\n",
        "    'name',\n",
        "    'genres',\n",
        "    'price',\n",
        "    'short_description',\n",
        "    'detailed_description',\n",
        "    'positive', # positive åˆ—ã‚’ä¿æŒ\n",
        "    'negative',\n",
        "    'developers',\n",
        "    'reviews',\n",
        "    'about_the_game',\n",
        "    'supported_languages'\n",
        "]\n",
        "\n",
        "# --- å‡¦ç† ---\n",
        "\n",
        "def process_csv_for_date_and_positive_filter():\n",
        "    \"\"\"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã¨positiveãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã—ã¦ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚\"\"\"\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« '{INPUT_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    print(f\"1. ãƒ•ã‚¡ã‚¤ãƒ« '{INPUT_FILE}' ã‚’èª­ã¿è¾¼ã¿ã€å¿…è¦ãªåˆ—ã‚’æŠ½å‡ºã—ã¾ã™...\")\n",
        "    try:\n",
        "        # 1. å¿…è¦ãªåˆ—ã ã‘ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ (usecolsã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)\n",
        "        # 'release_date'åˆ—ã¯å¿…ãšå«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
        "        cols_to_use = [col for col in COLUMNS_TO_KEEP if col in pd.read_csv(INPUT_FILE, encoding=ENCODING, nrows=1).columns]\n",
        "        if 'release_date' not in cols_to_use:\n",
        "             print(f\"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã« 'release_date' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚\", file=sys.stderr)\n",
        "             return\n",
        "        # positive åˆ—ãŒå¿…è¦ãªå ´åˆã¯è¿½åŠ \n",
        "        if 'positive' not in cols_to_use and 'positive' in COLUMNS_TO_KEEP:\n",
        "             cols_to_use.append('positive')\n",
        "\n",
        "\n",
        "        df = pd.read_csv(INPUT_FILE, encoding=ENCODING, usecols=cols_to_use)\n",
        "\n",
        "        # å‡¦ç†é–‹å§‹æ™‚ã®å…ƒã®è¡Œæ•°ã‚’è¨˜éŒ²\n",
        "        original_rows_count = len(df)\n",
        "        current_rows_count = original_rows_count\n",
        "\n",
        "        # 2. æ—¥ä»˜åˆ—ã®å¤‰æ›ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "        if 'release_date' in df.columns:\n",
        "            print(f\"2. æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ '{CUTOFF_DATE}' ã‚’é©ç”¨ã—ã¾ã™ (ã“ã®æ—¥ä»˜ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ®‹ã—ã¾ã™)...\")\n",
        "            # 'release_date'åˆ—ã®NaNã‚’é™¤å¤–ã—ã€å‹ã‚’å¤‰æ›\n",
        "            df.dropna(subset=['release_date'], inplace=True)\n",
        "            df['release_date'] = pd.to_datetime(df['release_date'])\n",
        "\n",
        "            cutoff_datetime = pd.to_datetime(CUTOFF_DATE)\n",
        "\n",
        "            # ãƒªãƒªãƒ¼ã‚¹æ—¥ãŒCUTOFF_DATEã‚ˆã‚Šã‚‚æ–°ã—ã„è¡Œã‚’æŠ½å‡º\n",
        "            df_filtered_date = df[df['release_date'] >= cutoff_datetime].copy() # >= ã«å¤‰æ›´ã—ã¦2022å¹´ã‚’å«ã‚€ã‚ˆã†ã«ã™ã‚‹\n",
        "            rows_dropped_by_date = current_rows_count - len(df_filtered_date)\n",
        "            current_rows_count = len(df_filtered_date)\n",
        "            print(f\"   -> æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã§ {rows_dropped_by_date} è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚\")\n",
        "            print(f\"   -> æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®è¡Œæ•°: {current_rows_count} è¡Œ\")\n",
        "        else:\n",
        "            print(\"âš ï¸ 'release_date' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
        "            df_filtered_date = df.copy() # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãŒãªã„å ´åˆã¯dfå…¨ä½“ã‚’ã‚³ãƒ”ãƒ¼\n",
        "\n",
        "\n",
        "        # 3. positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "\n",
        "        if 'positive' in df_filtered_date.columns:\n",
        "             print(f\"3. positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ({MIN_POSITIVE_REVIEWS}ä»¶ä»¥ä¸Š) ã‚’é©ç”¨ã—ã¾ã™...\")\n",
        "             # positive åˆ—ã‚’æ•°å€¤å‹ã«å¤‰æ›ã—ã€NaNã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "             df_filtered_date['positive'] = pd.to_numeric(df_filtered_date['positive'], errors='coerce')\n",
        "             df_filtered_date.dropna(subset=['positive'], inplace=True)\n",
        "\n",
        "             df_filtered_final = df_filtered_date[df_filtered_date['positive'] >= MIN_POSITIVE_REVIEWS].copy()\n",
        "             rows_dropped_by_positive = current_rows_count - len(df_filtered_final)\n",
        "             current_rows_count = len(df_filtered_final)\n",
        "             print(f\"   -> positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ã§ {rows_dropped_by_positive} è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚\")\n",
        "             print(f\"   -> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®æœ€çµ‚è¡Œæ•°: {current_rows_count} è¡Œ\")\n",
        "        else:\n",
        "             print(\"âš ï¸ 'positive' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
        "             df_filtered_final = df_filtered_date.copy() # positive åˆ—ãŒãªã„å ´åˆã¯æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®DataFrameã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
        "\n",
        "\n",
        "        # 4. æœ€çµ‚çš„ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "        print(f\"4. æœ€çµ‚çš„ãªDataFrameã‚’ '{OUTPUT_FILE}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã™...\")\n",
        "        # COLUMNS_TO_KEEPã§æŒ‡å®šã—ãŸåˆ—ã‚’ãã®ã¾ã¾ä¿å­˜\n",
        "        df_filtered_final.to_csv(OUTPUT_FILE, index=False, encoding=ENCODING)\n",
        "\n",
        "        # ä¿å­˜å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª\n",
        "        final_size_bytes = os.path.getsize(OUTPUT_FILE)\n",
        "        final_size_mb = final_size_bytes / (1024 * 1024)\n",
        "        print(f\"   -> ãƒ•ã‚¡ã‚¤ãƒ« '{OUTPUT_FILE}' ã‚’ä¿å­˜ã—ã¾ã—ãŸ (ã‚µã‚¤ã‚º: {final_size_mb:.2f} MB).\")\n",
        "        print(\"âœ… CSVå‡¦ç†å®Œäº†ã€‚\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{INPUT_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", file=sys.stderr)\n",
        "\n",
        "# é–¢æ•°ã‚’å®Ÿè¡Œ\n",
        "process_csv_for_date_and_positive_filter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l81ZGvlB5uc-",
        "outputId": "cdfefd30-2e09-4fc0-ed3b-7bf04451f673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ãƒ•ã‚¡ã‚¤ãƒ« 'games_march2025_cleaned.csv' ã‚’èª­ã¿è¾¼ã¿ã€å¿…è¦ãªåˆ—ã‚’æŠ½å‡ºã—ã¾ã™...\n",
            "2. æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ '2015-01-01' ã‚’é©ç”¨ã—ã¾ã™ (ã“ã®æ—¥ä»˜ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ®‹ã—ã¾ã™)...\n",
            "   -> æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã§ 2771 è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚\n",
            "   -> æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®è¡Œæ•°: 86847 è¡Œ\n",
            "3. positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (7000ä»¶ä»¥ä¸Š) ã‚’é©ç”¨ã—ã¾ã™...\n",
            "   -> positive ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ã§ 85465 è¡Œã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚\n",
            "   -> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®æœ€çµ‚è¡Œæ•°: 1382 è¡Œ\n",
            "4. æœ€çµ‚çš„ãªDataFrameã‚’ 'steam_games_filtered_final.csv' ã¨ã—ã¦ä¿å­˜ã—ã¾ã™...\n",
            "   -> ãƒ•ã‚¡ã‚¤ãƒ« 'steam_games_filtered_final.csv' ã‚’ä¿å­˜ã—ã¾ã—ãŸ (ã‚µã‚¤ã‚º: 6.17 MB).\n",
            "âœ… CSVå‡¦ç†å®Œäº†ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LangChain ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ã®å®šç¾©"
      ],
      "metadata": {
        "id": "r8khopZxhvSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92GACOyY7Q9o",
        "outputId": "fb709ec5-c3af-4736-ac00-0e2ccbde157d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List\n",
        "from pydantic import Field, model_validator # Pydanticé–¢é€£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "\n",
        "class LocalLLM(LLM):\n",
        "    # Pydantic v2 ã®è¨­å®š: extra='allow' ã§è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¨±å¯\n",
        "    model_config = {'extra': 'allow'}\n",
        "\n",
        "    system_prompt: Optional[str] = Field(default=None) # system_promptã‚’æ˜ç¤ºçš„ã«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦å®šç¾©\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ±ç”¨çš„ãªã‚‚ã®ã«å¤‰æ›´\n",
        "        if self.system_prompt:\n",
        "            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆ\n",
        "            full_prompt = f\"<s>[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
        "        else:\n",
        "            full_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "\n",
        "        output = llm_pipeline(full_prompt, max_new_tokens=1024, do_sample=True)[0][\"generated_text\"]\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¨ [/INST] ã‚’å‰Šé™¤ã—ã¦Assistantã®å¿œç­”ã®ã¿ã‚’æŠ½å‡º\n",
        "        # ELYZAãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´\n",
        "        assistant_prefix = \"[/INST]\"\n",
        "        if assistant_prefix in output:\n",
        "            return output.split(assistant_prefix, 1)[1].strip()\n",
        "        else:\n",
        "            # ã‚‚ã—[/INST]ãŒå‡ºåŠ›ã«å«ã¾ã‚Œãªã„å ´åˆã¯ã€å‡ºåŠ›å…¨ä½“ã‚’è¿”ã™ã‹ã€èª¿æ•´ãŒå¿…è¦\n",
        "            return output.strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"local-llm\" # ã‚¿ã‚¤ãƒ—åã‚’æ±ç”¨çš„ã«å¤‰æ›´\n",
        "\n",
        "# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã—ã¦LLMã‚’åˆæœŸåŒ–\n",
        "# ã“ã“ã«ã‚ãªãŸã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜å…¥ã—ã¦ãã ã•ã„\n",
        "system_prompt = \"å¿…ãšæ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼å‰ææ¡ä»¶ã®ã¿ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ•°ã¨ä¾¡æ ¼ï¼Œã‚²ãƒ¼ãƒ ã®èª¬æ˜ã‚’å¿…ãšè¡Œãªã£ã¦ãã ã•ã„ï¼ãƒ¦ãƒ¼ã‚¶ã®è³¼è²·æ„æ¬²ã‚’æ»ãç«‹ã¦ã‚‹ã‚ˆã†ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼\"\n",
        "llm = LocalLLM(system_prompt=system_prompt)\n",
        "\n",
        "print(\"LocalLLM with system prompt is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12bcbQMb7TVv",
        "outputId": "8b9abc0d-792c-46b3-d6dd-9dc7794fc619"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLLM with system prompt is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. RAG ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰"
      ],
      "metadata": {
        "id": "2Ir0RgjSh_Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os # ãƒ‡ãƒãƒƒã‚°ç”¨ã«osã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ã€Step 0: ç’°å¢ƒè¨­å®šã¨LLMã®æº–å‚™ã€‘\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# GPU ãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãƒ‡ãƒã‚¤ã‚¹ã‚’æ±ºå®š\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"âœ… Running on {device}: GPU will be used for embedding.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(f\"âš ï¸ Running on {device}. GPU not found, using CPU for embedding.\")\n",
        "\n",
        "try:\n",
        "    llm # llmå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "except NameError:\n",
        "    llm = None\n",
        "    print(\"âš ï¸ LLM (llmå¤‰æ•°) ã¯å®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚RetrievalQAãƒã‚§ãƒ¼ãƒ³ã¯æ§‹ç¯‰ã§ãã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ã€Step 1: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²ã€‘\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# æŒ‡å®šã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ (ãƒ•ã‚¡ã‚¤ãƒ«åã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã•ã‚Œã¦ã„ã‚‹å‰æ)\n",
        "csv_file_path = \"steam_games_filtered_final.csv\"\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"âŒ Error: CSV file not found at {csv_file_path}. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"1. Loading document from {csv_file_path}...\")\n",
        "loader = CSVLoader(csv_file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"   -> Document split into {len(chunks)} chunks.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ã€Step 2: ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨Chroma VectorStoreã®æ§‹ç¯‰ã€‘\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"2. Initializing SentenceTransformer on {device}...\")\n",
        "embedding = SentenceTransformerEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\",\n",
        "    model_kwargs={\"device\": device},\n",
        "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯32)\n",
        "    encode_kwargs={'batch_size': 64} # é©å®œèª¿æ•´ã—ã¦ãã ã•ã„\n",
        ")\n",
        "\n",
        "print(\"3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "print(\"   -> VectorStore built successfully.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ã€Step 3: Retrieval QA ãƒã‚§ãƒ¼ãƒ³ä½œæˆã€‘\n",
        "# ----------------------------------------------------\n",
        "\n",
        "if llm is not None:\n",
        "    # Retrieval QA ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    print(\"4. RetrievalQA chain ready. You can now use qa.run('Your question here')\")\n",
        "else:\n",
        "    print(\"4. RetrievalQA chain skipped because LLM (llm variable) is not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjTSAj37pwY",
        "outputId": "13e7867f-80d5-4aaa-b3dc-4f52bc87bd25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Running on cuda: GPU will be used for embedding.\n",
            "1. Loading document from steam_games_filtered_final.csv...\n",
            "   -> Document split into 1450 chunks.\n",
            "2. Initializing SentenceTransformer on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3510208420.py:53: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = SentenceTransformerEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. Creating Chroma VectorStore (Embedding phase started, GPU may be busy)...\n",
            "   -> VectorStore built successfully.\n",
            "4. RetrievalQA chain ready. You can now use qa.run('Your question here')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.å‡ºåŠ›"
      ],
      "metadata": {
        "id": "8_n7ENgqiCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "query = \"æ—¥æœ¬èªå¯¾å¿œã§ï¼Œãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½ãªè©•ä¾¡ã®é«˜ã„ã‚²ãƒ¼ãƒ ã‚’æ•™ãˆã¦\" #Queryã¯RAGã«èª­ã¿è¾¼ã¾ã›ãŸå†…å®¹ã«å¿œã˜ã¦å„è‡ªã§å¤‰æ›´ã€‚RAGã«èª­ã¿è¾¼ã¾ã›ãŸçŸ¥è­˜ã«é–¢ã™ã‚‹å•ã„åˆã‚ã›ã‚’ã™ã‚‹ã€‚\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "try:\n",
        "    response = qa.run(query)\n",
        "    print(\"å›ç­”:\", response)\n",
        "except RuntimeError as e:\n",
        "    print(\"CUDAãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å¯¾å‡¦æ¡ˆ:\")\n",
        "    print(\"- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’ç¸®å°\")\n",
        "    print(\"- ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™\")\n",
        "    print(\"- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«å›ºå®š\")\n",
        "    print(f\"è©³ç´°: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxyNLipPCEWx",
        "outputId": "75b348bf-5960-484e-fb3a-375b6ae7dd9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3925162713.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å›ç­”: <think>\n",
            "ã¾ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã§ã€ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½ãªè©•ä¾¡ã®é«˜ã„ã‚²ãƒ¼ãƒ ã‚’çŸ¥ã‚ŠãŸã„ã¨ç†è§£ã—ã¾ã—ãŸã€‚ã¾ãšã€æä¾›ã•ã‚ŒãŸ4ã¤ã®ã‚²ãƒ¼ãƒ ã®æƒ…å ±ã‚’è¦‹ã‚‹ã¨ã€Name: é­”å¥³çš„å¤œå®´ã€Price: 34.99ã€ supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese']ã€positive: 8085ã€negative: 110ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã¯ã€3äººã§ãƒ—ãƒ¬ã‚¤å¯èƒ½ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©•ä¾¡ãŒå¾ˆé«˜ãã€positively 8085ä»¶ã€negative 110ä»¶ã§ã™ã€‚ã“ã®(game)ãŒãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
            "\n",
            "æ¬¡ã«ã€Name: Sanfuã€Price: 6.59ã€ supported_languages: ['Simplified Chinese']ã€positive: 13481ã€negative: 2821ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã¯ã€2äººã§ãƒ—ãƒ¬ã‚¤å¯èƒ½ã§ã€positively 13481ä»¶ã€negative 2821ä»¶ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€support languageã¯Simplified Chineseã®ã¿ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã‚’æ”¯æŒã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®¹æ˜“ã§ã™ã€‚\n",
            "\n",
            "Name: OMORIã€Price: 19.99ã€ supported_languages: ['English', 'Japanese', 'Simplified Chinese', 'Korean']ã€positive: 75941ã€negative: 2144ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã¯ã€ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½æ€§ãŒã‚ã‚Šã€positively 75941ä»¶ã€negative 2144ä»¶ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ”¯æŒ language ã¯å¤šå…ƒä¸€ oats,ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã‚’æ”¯æŒã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®¹æ˜“ã§ã™ã€‚\n",
            "\n",
            "Name: Senrenï¼ŠBankaã€Price: 19.94ã€ supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese', 'English']ã€positive: 29679ã€negative: 330ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã¯ã€ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½æ€§ãŒã‚ã‚Šã€positively 29679ä»¶ã€negative 330ä»¶ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ”¯æŒ language ã¯å¤šå…ƒä¸€ oats,ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã‚’æ”¯æŒã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®¹æ˜“ã§ã™ã€‚\n",
            "\n",
            "ã—ãŸãŒã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªå¯¾å¿œã§ã€ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤å¯èƒ½ãªè©•ä¾¡ã®é«˜ã„ã‚²ãƒ¼ãƒ ã‚’çŸ¥ã‚ŠãŸã„å ´åˆã€Name: é­”å¥³çš„å¤œå®´ã€Name: Sanfuã€Name: OMORIã€Name: Senrenï¼ŠBanka ãŒå€™è£œã§ã™ã€‚Name: é­”å¥³çš„å¤œå®´ã¯positively 8085ä»¶ã€Name: Sanfuã¯13481ä»¶ã€Name: OMORIã¯75941ä»¶ã€Name: Senrenï¼ŠBankaã¯29679ä»¶ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹ Game ã¨ã—ã¦ã€Name: é­”å¥³çš„å¤œå®´ãŒæœ€ã‚‚positively 8085ä»¶ã§æœ€ã‚‚highly rated ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ strongly recommend ã™ã‚‹ Game ã¨ã—ã¦ã€Name: é­”å¥³çš„å¤œå®´ãŒæœ€é©ã§ã™ã€‚\n",
            "</think>\n",
            "\n",
            "Name: é­”å¥³çš„å¤œå®´  \n",
            "Price: 34.99  \n",
            "supported_languages: ['Japanese', 'Simplified Chinese', 'Traditional Chinese']  \n",
            "positive: 8085  \n",
            "negative: 110  \n",
            "\n",
            " strongly recommend!  \n",
            "This game is a Japanese-style romantic visual novel with a high score in multiple awards for its art, music, and characters. It has a multi-player mode and is highly praised for its engaging gameplay and emotional story. The game's popularity among Japanese users is evident, with 8085 positive reviews and only 110 negative reviews. It's a must-play for fans of Japanese visual novels!\n"
          ]
        }
      ]
    }
  ]
}